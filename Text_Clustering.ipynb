{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Unsupervised Learing: Text Clustering</center>\n",
    "\n",
    "References:\n",
    "* http://brandonrose.org/clustering\n",
    "* https://www.csee.umbc.edu/~nicholas/clustering/tutorial.pdf\n",
    "* http://scikit-learn.org/stable/auto_examples/text/document_clustering.html\n",
    "* https://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf\n",
    "* http://infolab.stanford.edu/~ullman/mmds/ch7.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clustering vs. Classification\n",
    "* Clustering (Unsupervised): divide a set of objects into clusters (parts of the set) so that objects in the same cluster are similar to each other, and/or objects in different clusters are dissimilar.\n",
    "  * Representation of the objects\n",
    "  * Similarity/distance measure\n",
    "* Classifification (Supervised): group objects into predetermined categories\n",
    "  * Representation of the objects\n",
    "  * A training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why clustering\n",
    "* Understand conceptually meaningful groups of objects that share common characteristics \n",
    "* Provides an abstraction from individual data objects to the clusters in which those data objects reside\n",
    "* Uses of clustering\n",
    "  * Summarization\n",
    "  * Compression\n",
    "  * Efficiently finding nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Types of Clusterings \n",
    "* Different kinds of models (https://www.geeksforgeeks.org/different-types-clustering-algorithm/):\n",
    "  - Centroid models (partition): \n",
    "     - Similarity is derived as the closeness of a data point to the centroid of clusters. \n",
    "     - Flat partition, e.g. K-Means\n",
    "     <img src='centroid.png' width=\"40%\">\n",
    "  - Connectivity models (Hierarchical algorithms): \n",
    "     - Data points closer in data space exhibit more similarity to each other than the data points lying farther away.      \n",
    "     - Hierarchy of clusters, e.g. agglomerative clustering\n",
    "     <img src='connectivity.png' width=\"40%\">\n",
    "  - Distribution models: \n",
    "     - How probable is it that all data points in the cluster belong to the same distribution, concept, or topic\n",
    "     - e.g. Latent Semantics Analysis, Latent Dirichlet Allocation (LDA)\n",
    "     <img src='distribution.png' width=\"40%\">\n",
    "  - Density models: clusters correspond to areas of varied density of data points in the data space\n",
    "     - e.g. DBSCAN\n",
    "     <img src='density.png' width=\"40%\">\n",
    "* Exclusive vs. Overlapping\n",
    "  - Exclusive: each object is assigned to a single cluster, e.g. K-Means\n",
    "  - Overlapping (non-exclusive): an object can simultaneously belong to more than one cluster, e.g. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation of Clustering: What is a good clustering\n",
    "### 4.1 External Evaluation: \n",
    "* External evaluation measures the degree to which predicted clustering labels correspond to actual class labels  \n",
    "* **Precision** and **Recall**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Internal Evaluation \n",
    "<img src='cohension_separation.png' width=\"60%\">\n",
    "* **Cohension (Intra-cluster similarity)**: how \"cohesive\" a cluster is, i.e. the average similarity of objects in the same cluster. \n",
    "   - e.g. cluster radius: $\\max{d(x, μ_A)}$ where $μ_A$ is the arithmetic mean of cluster A and $x$ is a point in A\n",
    "   - e.g. cluster diameter: $\\max{d(x, y)}$ where $x,y$ are two points in cluster A\n",
    "\n",
    "* **Separation (Inter-cluster dissimilarity)**: how \"separate\" a cluster from another, i.e. the average similarity of all samples in cluster $A$ to all the samples in cluster $B$.\n",
    "   - e.g. Separation can be calculated as average distance: $\\frac{1}{|A|*|B|}\\sum_{x \\in A}{\\sum_{y \\in B}{d(x, y)}}$ \n",
    "* Metrics with combined cohension and separation (http://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation)\n",
    "   - Silhouette Coefficient: $s=\\frac{b-a}{\\max(a,b)}$, where $a$: the mean distance between a sample and all other points in the same cluster, and $b$: the mean distance between a sample and all other points in the next nearest cluster. $s \\in [-1, 1]$.\n",
    "   - Calinski-Harabaz Index: $s=\\frac{b}{a}$ where $a$ is mean within\\-cluster separation, and $b$ is the mean between\\-cluster separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. K-Means\n",
    "### 5.1. Algorithm outline: Cluster objects into K clusters\n",
    "<img style=\"float: left;\" src='Kmean1.png'  width='20%'/><img  src='Kmean2.png' width='20%'/>\n",
    "- Algorithm: \n",
    "    1. Select K points as initial centroids \n",
    "    2. Repeat until centroids do not change:\n",
    "        1. Form K clusters by assigning each point to its closest centroid by distance.\n",
    "        2. Recompute the centroid of each cluster as the arithmetic mean of samples within the cluster. \n",
    "- A few observations of K-means:\n",
    "  - Initial centroids have an impact on clustering. Usually, several rounds of clustering with random initial centroids are performed, and the most commonly occurring output centroids are chosen.\n",
    "  - Centroids and distance measure are crtical in the algorithm\n",
    "     - **Euclidean distance**: \n",
    "       - the best centroid for minimizing the average distance from all samples to the centroid is the mean of points in the cluster (https://www-users.cs.umn.edu/~kumar001/dmbook/ch8.pdf)\n",
    "     - **Cosine similarity**: \n",
    "       * Well-accepted similarity measure for documents\n",
    "       * It is not guaranteed that the mean of samples in a cluster is the best centroid \n",
    "       * For text clustering, the centroid does not stand for an actual document. How to interpret clusters?\n",
    "       * A modified version of Kmeans is called K-medoids, where a representative sample is choosen as the center of a cluster, called as a medoid.   \n",
    "- Python packages for Kmean\n",
    "  * NLTK: can choose Euclidean or Cosine similarity as distance measure\n",
    "  * Sklearn: only Euclidean distance is supported  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1.1 Load data and generate TF-IDF\n",
    "# Load datasets (http://qwone.com/~jason/20Newsgroups/)\n",
    "# For convenience, a subset of the data has been saved into \"twenty_news_data.pkl\"\n",
    "\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "data=pickle.load(open(\"twenty_news_data.pkl\",\"rb\"))\n",
    "\n",
    "#Load just three categories: comp.graphics and soc.religion.christian\n",
    "data=[item for item in data if item[1] in \\\n",
    "      ['comp.graphics','soc.religion.christian', 'sci.med']]\n",
    "\n",
    "# Separate list of tuples to two lists\n",
    "text,target=zip(*data)\n",
    "text=list(text)\n",
    "target=list(target)\n",
    "\n",
    "# initialize the TfidfVectorizer \n",
    "# set min document frequency to 5\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(stop_words=\"english\",\\\n",
    "                             min_df=5) \n",
    "\n",
    "# generate tfidf matrix\n",
    "dtm= tfidf_vect.fit_transform(text)\n",
    "print (dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1.2 Clustering using NLTK KMean\n",
    "\n",
    "from nltk.cluster import KMeansClusterer, cosine_distance\n",
    "\n",
    "# set number of clusters\n",
    "num_clusters=3\n",
    "\n",
    "# initialize clustering model\n",
    "# using cosine distance\n",
    "# clustering will repeat 10 times\n",
    "# each with different initial centroids\n",
    "clusterer = KMeansClusterer(num_clusters, \\\n",
    "                            cosine_distance, repeats=10)\n",
    "\n",
    "# samples are assigned to cluster labels starting from 0\n",
    "clusters = clusterer.cluster(dtm.toarray(), \\\n",
    "                             assign_clusters=True)\n",
    "\n",
    "#print the cluster labels of the first 5 samples\n",
    "print(clusters[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1.3 Interpret each cluster by centroid\n",
    "\n",
    "# a centroid is the arithemtic mean \n",
    "# of all samples in the cluster\n",
    "# it may not stand for a real document\n",
    "\n",
    "# find top words at centroid of each cluster\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# clusterer.means() contains the centroids\n",
    "# each row is a cluster, and \n",
    "# each column is a feature (word)\n",
    "centroids=np.array(clusterer.means())\n",
    "\n",
    "# argsort sort the matrix in ascending order \n",
    "# and return locations of features before sorting\n",
    "# [:,::-1] reverse the order\n",
    "sorted_centroids = centroids.argsort()[:, ::-1] \n",
    "\n",
    "# The mapping between feature (word)\n",
    "# index and feature (word) can be obtained by\n",
    "# the vectorizer's function get_feature_names()\n",
    "voc_lookup= tfidf_vect.get_feature_names()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    \n",
    "    # get words with top 20 tf-idf weight in the centroid\n",
    "    top_words=[voc_lookup[word_index] \\\n",
    "               for word_index in sorted_centroids[i, :20]]\n",
    "    print(\"Cluster %d: %s \" % (i, \"; \".join(top_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. How to evaluate clustering\n",
    "- External evaluation:\n",
    "  - Obtain \"ground truth\": if data is not labeled, manually label a random subset of samples as \"ground truth\" \n",
    "  - Assign each cluster to a \"true\" class by the **majority vote rule**, for example:\n",
    "  - Calculate precision and recall\n",
    "  \n",
    "  \n",
    "  | Cluster ID      | Ground Truth Class Label   |\n",
    "  | :------------- |:----------------------------|\n",
    "  | 0      | comp.graphics|\n",
    "  | 1      | sci.med  |\n",
    "  | 2      | soc.religion.christian|\n",
    "  \n",
    "- Internal evaluation\n",
    "  - Silhouette Coefficient\n",
    "  - Calinski-Harabaz Index\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.2.1 External evaluation\n",
    "# determine cluster labels and calcuate precision and recall\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame(list(zip(target, clusters)), \\\n",
    "                columns=['actual_class','cluster'])\n",
    "df.head()\n",
    "pd.crosstab( index=df.cluster, columns=df.actual_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict={0:'comp.graphics', 1:\"sci.med\",\\\n",
    "              2:'soc.religion.christian'}\n",
    "\n",
    "# Assign true class to cluster\n",
    "predicted_target=[cluster_dict[i] for i in clusters]\n",
    "\n",
    "print(metrics.classification_report\\\n",
    "      (target, predicted_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Clustering with sklearn package - Euclidean distance\n",
    "- Compare its performance with NLTK Kmeans result\n",
    "- Discuss: the difference between performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.3.1 Clustering with sklearn package - Euclidean distance\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Kmeans with 10 different centroid seeds\n",
    "km = KMeans(n_clusters=num_clusters, n_init=10).fit(dtm)\n",
    "clusters = km.labels_.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.3.2 Performance Evaluation\n",
    "\n",
    "df=pd.DataFrame(list(zip(target, clusters)), \\\n",
    "                columns=['actual_class','cluster'])\n",
    "df.head()\n",
    "pd.crosstab( index=df.cluster, columns=df.actual_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_dict={0:'comp.graphics', 1:\"sci.med\",\\\n",
    "              2:'soc.religion.christian'}\n",
    "\n",
    "# Assign true class to cluster\n",
    "predicted_clusters=[cluster_dict[i] for i in clusters]\n",
    "\n",
    "print(metrics.classification_report\\\n",
    "      (target, predicted_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.3.3 Add \"sci.atheism\" documents to \n",
    "# the samples and re-do clustering\n",
    "# Steps:\n",
    "# 1. add \"alt.atheism\" to line 13 of Exercise 5.1.1\n",
    "# 2. change the number of clusters to 4 in line 6 of Exercise 5.1.2\n",
    "# 3. Re-run Exercise 5.1.1 and Exercise 5.1.2\n",
    "# 4. In Exercise 5.2.1, change the mapping between clusters \n",
    "#    and true class and recalculate performance\n",
    "\n",
    "# 5. Accordingly, try Exercise 5.3.1 and 5.3.2 to see\n",
    "#    the performance under Euclidean distance   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. How to pick *K*, the number of clusters?\n",
    "- **Try internal valuation first!!!**\n",
    "  - manually assess a subset of documents to create \"ground truth\"\n",
    "- In case there is impossible to figure out how many clusters in the data set manually, **theorectically**, *K* may be selected as follows:\n",
    "  * Select a metric to measure the \"goodness\" of clusters, e.g. average radius, average diameter, etc.\n",
    "  * Varying *K* from 2 to N, perform clustering for each *K*\n",
    "  * Ideally, as *K* increases to some point, the metric should grow slowly (**elbow method**)\n",
    "\n",
    "<img style=\"float: left;\" src='best_k.png'  width='40%'/><img  src='sample1.png' width='30%'/>\n",
    "source: http://infolab.stanford.edu/~ullman/mmds/ch7.pdf\n",
    "- However, if samples do not have clear structures, this method may not work (elbow does not exist!)\n",
    "<img src=\"samples2.png\" width='30%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agglomerative Hierarchical Clustering\n",
    "* Steps (bottom-up):\n",
    "    - Starting with each point as a singleton cluster\n",
    "    - Repeatedly merging the two closest clusters until a single, all-encompassing cluster remains\n",
    "* The clustering result is represented as a dendrogram\n",
    "<img src='dendrogram.png' width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 6.1 Hierarchical clustering\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage,dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# distance between any pair of documents\n",
    "dist = 1 - cosine_similarity(dtm)\n",
    "\n",
    "# create a linkage matrix using ward's method\n",
    "# the linkage matrix contains the dendragram data\n",
    "# in ward methods, the distance between two clusters\n",
    "# is measured as the sum of distance \n",
    "# from each sample to the cluster center \n",
    "\n",
    "#linkage_matrix = average(dist)\n",
    "linkage_matrix = linkage(dist, method='ward')\n",
    "\n",
    "# plot the dendrogram\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"top\");\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 6.2 Get Clusters\n",
    "\n",
    "from scipy.cluster.hierarchy import cut_tree\n",
    "\n",
    "clusters = cut_tree(linkage_matrix,height=12)\n",
    "clusters=clusters.flatten().tolist()\n",
    "\n",
    "# get documents from each clusters and examine the clusters\n",
    "\n",
    "df=pd.DataFrame(list(zip(target, clusters)), \\\n",
    "                columns=['actual_class','cluster'])\n",
    "df.head()\n",
    "pd.crosstab( index=df.cluster, columns=df.actual_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Exercise 6.3 Map clusters to true class\n",
    "\n",
    "cluster_dict={0:'comp.graphics', 2:\"sci.med\",\\\n",
    "              1:'soc.religion.christian', 3:\"sci.med\",\\\n",
    "              4:\"sci.med\",5:'soc.religion.christian'}\n",
    "\n",
    "# Assign true class to cluster\n",
    "# Some small clusters cannot be mapped to any true class\n",
    "predicted_target=[cluster_dict[i] for i in clusters ]\n",
    "\n",
    "                     \n",
    "print(metrics.classification_report\\\n",
    "      (target, predicted_target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
