{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Supervised Learning - Text Classification</center>\n",
    "References:\n",
    "* http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What we'll cover in this lecture\n",
    "* Review basic concepts of machine learning\n",
    "  * Cross validation\n",
    "  * Performance metrics: recall and precision\n",
    "* Text Classification  \n",
    "  * Assign a document into one  or more pre-defined categories (or labels)\n",
    "    * Input: \n",
    "      - a document $d$ \n",
    "      - a fixed set of classes C = {$c_1$, $c_2$,..., $c_J$}\n",
    "      - A training set of $m$ hand-labeled documents ($d_1,c_1$),....,($d_m,c_m$)\n",
    "    * Output: a classifier that predicts $d$ to some classes $c$ $\\subset$ C\n",
    "  * **Single-label** classification: e.g. spam dection, sentiment detection\n",
    "  * **Multi-label** classification: e.g. news categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review basic concepts of machine learning\n",
    "### 2.1. Cross validation \n",
    "  * Defition: model valiation technique for assessing how a model will generalize to an independent data set\n",
    "  * *k*-fold cross validation: \n",
    "    - Often, there is not enough data to allow some of it to be kept back for testing. \n",
    "    - Thus, the data is separated into k subsets. Each time, one of the subsets is held as the test set (a.k.a holdout) and the rest of them is used as the training set. \n",
    "    - This method repeats *k* times and each time with a different subset as the test set. \n",
    "  <img src=\"cross_validation.png\" width=\"40%\"> [source] (http://spark-public.s3.amazonaws.com/nlp/slides/sentiment.pptx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Performance metrics\n",
    "  * Precision: precentage of true cases among the predicated true cases\n",
    "  * Recall:  precentage of true cases that have been retrieved over the total number of true cases\n",
    "  * F-score: $$\\frac{2*precision*recall}{precision+recall}$$\n",
    "  * Example: \n",
    "Confusion Matrix: <img src=\"confusion_matrix.png\">\n",
    "    * For \"YES\" group: \n",
    "      - precision=?, \n",
    "      - recall=?, \n",
    "      - f-score=?\n",
    "      <img src=\"precision_recall.png\" width=\"60%\">\n",
    "    * For \"NO\" group:\n",
    "      - precision=?, \n",
    "      - recall=?, \n",
    "      - f-score=?\n",
    "  * Overall model performance\n",
    "    * precision_macro (or recall_macro or f1_macro) is calculated as:\n",
    "      1. calculate precision for each label\n",
    "      2. average over labels \n",
    "    * precision_micro (or recall_micro or f1_micro): calculates metrics globally regardless of labels\n",
    "    * With inbalanced classes, the difference between these two metrics may be significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Basic process\n",
    "  1. Load and preprocess sample data\n",
    "  2. Extract features: e.g. bag of words with TF-IDF weights\n",
    "  3. Split data into trainning and test sets following cross validation method\n",
    "  4. Train a classifier/model with the training dataset using selected classification algorithm for each fold\n",
    "  5. Calculate performance\n",
    " \n",
    "* Considerations for deciding text classification algorithms\n",
    "  - should be effective in high dimensional spaces\n",
    "  - should be effective even if the number of features is greater than the number of samples\n",
    "    * Is regression a good alogorithm if you have a small number of text samples?\n",
    "  - some good algorithms to start with:\n",
    "      - Naive Bayes (https://web.stanford.edu/class/cs124/lec/naivebayes.pdf): baseline for performance benchmarking of text classification algorithms\n",
    "      - Support Vector Machine (SVM) (https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.1.: Load data \n",
    "# Load datasets (http://qwone.com/~jason/20Newsgroups/)\n",
    "# For convenience, a subset of the data has been saved into \"twenty_news_data.pkl\"\n",
    "\n",
    "import pickle\n",
    "data=pickle.load(open(\"twenty_news_data.pkl\",\"rb\"))\n",
    "\n",
    "# data is a list of tuple (text, label)\n",
    "# print out the first document\n",
    "print(\"Text: \"+data[0][0]+\"\\nGroup: \"+data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.2. Prepare text and class labels\n",
    "\n",
    "# split data as list of tuples into text and target tuples\n",
    "text,target=zip(*data)\n",
    "\n",
    "# convert tuple to list\n",
    "text=list(text)\n",
    "target=list(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. TF-IDF matrix generation\n",
    "- Function: **sklearn.feature_extraction.text.TfidfVectorizer**(input='content',encoding='utf-8', decode_error='strict', token_pattern='(?u)\\b\\w\\w+\\b', lowercase=True, stop_words=None, ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, norm='l2', use_idf=True, smooth_idf=True, ...)\n",
    "- Some useful parameters:\n",
    "    * **input** : string {'filename', 'file', 'content').\n",
    "    * **encoding** : string, 'utf-8' by default.\n",
    "If bytes or files are given to analyze, this encoding is used to decode.\n",
    "    * **decode_error** : {'strict', 'ignore', 'replace'}: Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding. By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are ‘ignore’ and ‘replace’.\n",
    "    * **token_pattern** : Regular expression denoting what constitutes a “token”. The default is '(?u)\\b\\w\\w+\\b', i.e. a token contains at least two word characters in unicode (note: ?u: unicode, \\b: space or non-word character, i.e. boundary, \\w: word character). \n",
    "    * **ngram_range** : tuple (min_n, max_n): The lower and upper boundary of the range of n-values for different n-grams to be extracted. \n",
    "    * **stop_words** : string {‘english’}, list, or None (default)\n",
    "    * **lowercase** : boolean, default True: Convert all characters to lowercase before tokenizing.\n",
    "    * **max_df/min_df** : float in range [0.0, 1.0] or int, default=1.0: When building the vocabulary ignore terms that have a document frequency strictly higher (lower) than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. \n",
    "    * **max_features** : int or None, default=None. If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "    * **norm** : 'l1', 'l2' or None, optional. Norm used to normalize term vectors. None for no normalization.\n",
    "    * **use_idf** : boolean, default=True. Enable inverse-document-frequency reweighting.\n",
    "    * **smooth_idf** : boolean, default=True. Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n",
    "- For all the parameters, see http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.3. Create TF-IDF Matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialize the TfidfVectorizer \n",
    "\n",
    "tfidf_vect = TfidfVectorizer() \n",
    "\n",
    "# with stop words removed\n",
    "#tfidf_vect = TfidfVectorizer(stop_words=\"english\") \n",
    "\n",
    "# generate tfidf matrix\n",
    "dtm= tfidf_vect.fit_transform(text)\n",
    "\n",
    "print(\"type of dtm:\", type(dtm))\n",
    "print(\"size of tfidf matrix:\", dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.4. Examine TF-IDF\n",
    "\n",
    "# 1. Check vocabulary\n",
    "# Vocabulary is a dictionary mapping a word to an index\n",
    "print(\"type of vocabulary:\", type(tfidf_vect.vocabulary_))\n",
    "print(\"index of word 'city' in vocabulary:\", \\\n",
    "      tfidf_vect.vocabulary_['city'])\n",
    "\n",
    "\n",
    "# get mapping from word index to word\n",
    "# i.e. reversal mapping of tfidf_vect.vocabulary_\n",
    "voc_lookup={tfidf_vect.vocabulary_[word]:word \\\n",
    "            for word in tfidf_vect.vocabulary_}\n",
    "\n",
    "# the number of words in the vocabulary\n",
    "print(\"total number of words:\", len(voc_lookup ))\n",
    "\n",
    "\n",
    "# 2. check words with top tf-idf wights in a document, e.g. 1st document\n",
    "\n",
    "print(\"\\nOriginal text: \\n\"+text[0])\n",
    "\n",
    "print(\"\\ntfidf weights: \\n\")\n",
    "\n",
    "# first, covert the sparse matrix row to a dense array\n",
    "doc0=dtm[0].toarray()[0]\n",
    "print(doc0.shape)\n",
    "\n",
    "# get index of top 20 words\n",
    "top_words=(doc0.argsort())[::-1][0:20]\n",
    "print([(voc_lookup[i], doc0[i]) for i in top_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.5. classification using a single fold\n",
    "\n",
    "# use MultinomialNB algorithm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# import method for split train/test data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import method to calculate metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "                dtm, target, test_size=0.3, random_state=0)\n",
    "\n",
    "# train a multinomial naive Bayes model using the testing data\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "# predict the news group for the test dataset\n",
    "predicted=clf.predict(X_test)\n",
    "\n",
    "# get the list of unique labels\n",
    "labels=sorted(list(set(target)))\n",
    "\n",
    "# calculate performance metrics. \n",
    "# Support is the number of occurrences of each label\n",
    "\n",
    "precision, recall, fscore, support=\\\n",
    "     precision_recall_fscore_support(\\\n",
    "     y_test, predicted, labels=labels)\n",
    "\n",
    "print(labels)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(fscore)\n",
    "print(support)\n",
    "\n",
    "# another way to get all performance metrics\n",
    "print(classification_report\\\n",
    "      (y_test, predicted, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.6.  predict new documents\n",
    "\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "\n",
    "# generate tifid for new documents\n",
    "X_new_tfidf = tfidf_vect.transform(docs_new)\n",
    "\n",
    "print(X_new_tfidf.shape)\n",
    "\n",
    "# predict classes for new documents\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for idx, doc in enumerate(docs_new):\n",
    "    print('%r => %s' % (doc, predicted[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.7. Classification with stop words removed\n",
    "# Can removing stop words improves performance?\n",
    "# In Exercise 3.3, uncomment line 10 and comment line 7\n",
    "# Run Exercise 3.3\n",
    "# Run Exercise 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.8. Run 5 fold cross validation\n",
    "# to show the generalizability of the model\n",
    "\n",
    "# import cross validation method\n",
    "from sklearn.model_selection import cross_validate\n",
    "#from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "metrics = ['precision_macro', 'recall_macro', \"f1_macro\"]\n",
    "\n",
    "#clf = MultinomialNB()\n",
    "clf = MultinomialNB(alpha=0.8)\n",
    "\n",
    "cv = cross_validate(clf, dtm, target, scoring=metrics, cv=5)\n",
    "print(\"Test data set average precision:\")\n",
    "print(cv['test_precision_macro'])\n",
    "print(\"\\nTest data set average recall:\")\n",
    "print(cv['test_recall_macro'])\n",
    "print(\"\\nTest data set average fscore:\")\n",
    "print(cv['test_f1_macro'])\n",
    "\n",
    "# To see the performance of training data set use \n",
    "# cv['train_xx_macro']\n",
    "print(cv['train_f1_macro'])\n",
    "\n",
    "# The metrics are quite stable across folds.\n",
    "# The performance between training and test sets is small\n",
    "# This indicates the model has good generalizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.9. Multinominal NB \n",
    "# with different smoothing parameter alpha\n",
    "# comment line 11 and uncomment 12 in Exercise 3.8\n",
    "# use different alpha value to see if it affects performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.10. SVM model\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "#from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import svm\n",
    "\n",
    "metrics = ['precision_macro', 'recall_macro', \"f1_macro\"]\n",
    "\n",
    "# initiate an linear SVM model\n",
    "clf = svm.LinearSVC()\n",
    "\n",
    "cv = cross_validate(clf, dtm, target, scoring=metrics, cv=5)\n",
    "print(\"Test data set average precision:\")\n",
    "print(cv['test_precision_macro'])\n",
    "print(\"\\nTest data set average recall:\")\n",
    "print(cv['test_recall_macro'])\n",
    "print(\"\\nTest data set average fscore:\")\n",
    "print(cv['test_f1_macro'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Parameter tuning using grid search\n",
    "* Each classification model has a few parameters\n",
    "  * e.g. \"stop_words\": \"english\" or None, min_df: [1,2,3, ...]\n",
    "  * e.g. MultinomialNB(alpha=1.0)\n",
    "  * e.g. LinearSVC(penalty=’l2’, loss=’squared_hinge’,...)\n",
    "* Instead of tweaking the parameters of the various components, it is possible to run an exhaustive search of the best parameters on a grid of possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.3.1 Grid search\n",
    "\n",
    "# import pipeline class\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# build a pipeline which does two steps all together:\n",
    "# (1) generate tfidf, and (2) train classifier\n",
    "# each step is named, i.e. \"tfidf\", \"clf\"\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                   ])\n",
    "\n",
    "# set the range of parameters to be tuned\n",
    "# each parameter is defined as \n",
    "# <step name>__<parameter name in step>\n",
    "# e.g. min_df is a parameter of TfidfVectorizer()\n",
    "# \"tfidf\" is the name for TfidfVectorizer()\n",
    "# therefore, 'tfidf__min_df' is the parameter in grid search\n",
    "\n",
    "parameters = {'tfidf__min_df':[2,5,10],\n",
    "              'tfidf__stop_words':[None,\"english\"],\n",
    "              'clf__alpha': [0.5,1.0,2.0],\n",
    "}\n",
    "\n",
    "# the metric used to select the best parameters\n",
    "metric =  \"f1_macro\"\n",
    "\n",
    "# GridSearch also uses cross validation\n",
    "gs_clf = GridSearchCV\\\n",
    "(text_clf, param_grid=parameters, scoring=metric, cv=5)\n",
    "\n",
    "# due to data volume and large parameter combinations\n",
    "# it may take long time to search for optimal parameter combination\n",
    "# you can use a subset of data to test\n",
    "gs_clf = gs_clf.fit(text, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gs_clf.best_params_ returns a dictionary \n",
    "# with parameter and its best value as an entry\n",
    "\n",
    "for param_name in gs_clf.best_params_:\n",
    "    print(param_name,\": \",gs_clf.best_params_[param_name])\n",
    "\n",
    "print(\"best f1 score:\", gs_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.3.2 Grid search\n",
    "# Modify Exercise 3.3 and Exercise 3.8 \n",
    "# to use the best parameters found\n",
    "# re-create the Multinominal NB classifier\n",
    "\n",
    "# also, check the dimension reduction of feature space by set min_df to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-label classification\n",
    "- So far we only cover single-label classification, i.e. assign one class to each sample\n",
    "- Multilabel classification emerges as a challenging problem, where classes are not mutually exclusive \n",
    "  * music categorization \n",
    "  * semantic classification of images\n",
    "  * tagging\n",
    "- **One-Vs-the-Rest** Strategy (a.k.a **one-vs-all**)\n",
    "  * fitting one classifier per class. For each classifier, the class is fitted against all the other classes.\n",
    "  * for $n$ classes (labels), $n$ classifier is needed\n",
    "  * Advantage: good interpretability - Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier\n",
    "  * Disadvantage: \n",
    "     * many classifiers are created if there is a large number classes\n",
    "     * ignore the structure (or dependencies) of classes\n",
    "- **Class indication matrix** (or **one-hot encoding**): Encode categorical integer features using a one-hot aka one-of-K scheme. \n",
    "\n",
    "| Document    | Money       | Investment | Crime & Justice |\n",
    "| :-----------|:-----------:|:----------:|:--------------:|\n",
    "| 1           | 0           |      0     | 1              |\n",
    "| 2           | 1           |      1     | 0              |\n",
    "| 3           | 1           |      0     | 0              |\n",
    "| 4           | 0           |      1     | 1              |\n",
    "\n",
    "- **dataset**: Yahoo News Ranked Multilabel Learning dataset (http://research.yahoo.com)\n",
    "  - A subset is selected\n",
    "  - 4 classes, 6426 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 4.1 Multi-label classification- Load data\n",
    "\n",
    "import json\n",
    "data=json.load(open(\"ydata.json\",\"r\"))\n",
    "\n",
    "docs,labels=zip(*data)\n",
    "\n",
    "# show sample examples\n",
    "docs[1]\n",
    "labels[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 4.2 One-hot coding of classes\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y=mlb.fit_transform(labels)\n",
    "# check size of indicator matrix\n",
    "Y.shape\n",
    "# check classes\n",
    "mlb.classes_\n",
    "\n",
    "# check # of samples in each class\n",
    "np.sum(Y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 4.3 Multi-label classification- one vs. rest classifier\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                docs, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=\"english\", min_df=2)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 4.4 Multi-label classification- Performance report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "predicted.shape\n",
    "predicted[0:2]\n",
    "Y_test[0:2]\n",
    "\n",
    "print(classification_report\\\n",
    "      (Y_test, predicted, target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
