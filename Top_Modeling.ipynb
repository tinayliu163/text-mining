{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Topic Modeling </center>\n",
    "\n",
    "- This lecture is created based on \n",
    "    * Blei. D. (2012), \"Probabilistic Topic Models\", http://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf\n",
    "    * Topic Modeling with Scikit Learn: https://medium.com/@aneesha/topic-modeling-with-scikit-learn-e80d33668730"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "- Topic modeling provides methods for automatically organizing, understanding, searching, and summarizing large electronic archives.\n",
    "  - Discover the hidden themes that pervade the collection.\n",
    "  - Annotate the documents according to those themes.\n",
    "  - Use annotations to organize, summarize, and search the texts.\n",
    "- Formal Definition\n",
    "<img src='topic_modeling.png' width='70%'>\n",
    "  - **Topics**: each topic is a **distribution over words** \n",
    "    - e.g. for topic \"Gentics\", $p('gene'~|~'Genetics')~=~0.04$, $p('dna'~|~'Genetics')=0.02$\n",
    "    - $K$ topics $\\theta_1, \\theta_2, ..., \\theta_K$, $N$ words $w_1, w_2, ..., w_N$ in corpus, we need to know  $p(w_i|\\theta_j)$ for $i \\in N$ and $j\\in K$\n",
    "  - **Document ($d$)**: a **mixture of topics**\n",
    "    - e.g. for above document $d$, $p('Genetics'~|~d)=0.5$, $p('LifeScience'~|~d)=0.15$, ...\n",
    "    - In general, given document $d$ and topic $\\theta_j$, we need to know $p(\\theta_j~|~d)$, i.e. **topic proportion**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Language Model\n",
    "- Definition: given a corpus with $M$ documents, $N$ words, $K$ topics, a model contains the following probabilities:\n",
    "  - topic probability distribution in corpus: <br>$p(\\theta_j)$ for $j \\in K$, $\\sum_{j\\in K}{p(\\theta_j)}=1$\n",
    "  - topic distribution per document $d$ (document assignment): <br>$p(\\theta_j~|~d)$, $\\sum_{j\\in K}{p(\\theta_j~|~d)}=1$ \n",
    "  - word distribution per topic (why do we need to know it?): <br> $p(w_i~|~\\theta_j)$ for $i \\in N$ and $j\\in K$, $\\sum_{i\\in N}{p(w_i~|~\\theta_j)}=1$ \n",
    "  <img src='language_model.png' width='30%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to estimate these probabilities?\n",
    "### 3.1. Supervised learning - Naive Bayes\n",
    "- Topic probability: \n",
    "$$ p(\\theta_j) = \\frac{\\text{documents in topic } j} {\\text{total documents}}$$  \n",
    "- Word distribution per topic: \n",
    "$$ p(w_i~|~\\theta_j)= \\frac{\\text{count of word } w_i \\text{ in topic } j} {\\text{total word count in documents of topic }j}$$  \n",
    "- Topic distribution per document: \n",
    "$$ \\begin{array}{l}\n",
    " p(\\theta_j~|~d) = \\frac{p(d|\\theta_j) * p(\\theta_j)}{p(d)} \\text{         # Bayesian rule}\\\\\n",
    " C_{MAP} = \\underset{\\theta}{\\operatorname{argmax}}{p(d~|~\\theta)*p(\\theta)} \\text{         # maximum a posteriori}\\\\\n",
    "    C_{MAP} = \\underset{\\theta}{\\operatorname{argmax}}{p(w_1,w_2, ...,w_N~|~\\theta)*p(\\theta)} \\\\\n",
    "    C_{MAP} = \\underset{\\theta}{\\operatorname{argmax}}({\\prod_{i \\in N} {p(w_i~|~\\theta)})*p(\\theta)}  \\textit{ # independence assumption}\n",
    "  \\end{array}$$  \n",
    "- Naive Bayes model is also a kind of language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Generative Model for Unsupervised learning \n",
    "- We don't have labeled data; we only observe the documents\n",
    "- We **cannot** estimate $p(\\theta_j)$ and $p(w_i~|~\\theta_j)$ as above\n",
    "- Instead, we use a **generative model** that describes how a document $d$ was created\n",
    "  1. decide on document length $N$, e.g. 100 words\n",
    "  2. decide on topic mixture (i.e. $p(\\theta_j~|~d)$), e.g. 70% about genetics and 30% about life science, ...\n",
    "  3. for each of the N words,\n",
    "     - 3.1. choose a topic from the topic mixture, e.g. \"genetics\"\n",
    "     - 3.2. choose a word from based on the probabilities of words in the topic (i.e. $p(w_i~|~\\theta_j)$), e.g. \"gene\"\n",
    "     - At the end, you may get a document such as \"gene dna life ...\"\n",
    "- We assume all documents in the dataset were generated following this process. Then we infer these probabilities from samples such that these probabilities have the maximum likelihood to generate the samples\n",
    "- Probabilities $p(w_i~|~\\theta_j)$ and $p(\\theta_j~|~d)$ are **hidden structures** to be discovered, a.k.a **latent variables**\n",
    "<img src='latent_structure.png' width='70%'>\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent Dirichlet Allocation (LDA)\n",
    "- A generative model which generates a document $d$ as follows:\n",
    "  1. Choose document length $N$ ∼ Poisson(ξ).\n",
    "  2. Choose topic mixture $\\theta$ ~ Dir(α).\n",
    "  3. For each of the $N$ words $w_n$:\n",
    "     - (a) Choose a topic assignment $z_n$ ∼ Multinomial(θ).\n",
    "     - (b) Choose a word $w_n$ from the topic, $z_n$ ∼ Multinomial($\\beta_{z_n}$), where $\\beta_{z_n}$ is the word distribution for assigned topic $z_n$, i.e. $p(w_n~|~z_n)$     \n",
    "- A few distributions\n",
    "  - Poisson(ξ) : a given number of events occurring in a fixed interval of time/space with rate ξ independently of the time/space since the last event\n",
    "  - Multinomial(θ) & Multinomial($\\beta$): \n",
    "    - suppose X is a vector which represents n draws of a random variable with three possible outcomes (i.e. words), say A, B, C. \n",
    "    - e.g. when n=10, an example draw of X could be x = [4,4,2], i.e., A occured 4 times, B 4 times, and C 2 times \n",
    "    - assume three outcomes have probability θ={$\\beta_A$, $\\beta_B$,$\\beta_C$} respectively (i.e. 0.5,0.3,0.2)\n",
    "    - the multinomial distribution describes the prob. mass distribution of X, $$ multinomial(X=[4,4,2]) = \\frac{10!}{4!4!2!}\\beta_A^{4}\\beta_B^{4}\\beta_C^{2}$$ \n",
    "  - Dir(α) : is a probability distribution with parameter $α, e.g. \\{α_1,α_2,α_3\\}$ to generate $θ, e.g. \\{ θ_1,θ_2,θ_3\\}$. For details of Dirichlet function, check videos e.g. https://www.youtube.com/watch?v=nfBNOWv1pgE \n",
    "  <img src='dirichlet.svg'>\n",
    "    - Dirichlet distribution is conjugate to the multinomial.\n",
    "    - Given a multinomial observation, the posterior distribution of θ is a Dirichlet.\n",
    "    - In LDA, usually $α_1=α_2=α_3=...=\\frac{1}{K}$\n",
    "- Common techniques to estimate these probabilities are EM (Expectation-Maximization), Gibbs Sampling (See Blei's paper for details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Estimate parameters by Gibbs Sampling\n",
    "- General ideas of Gibbs sampling\n",
    "  - In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult (from *Wikipedia*).\n",
    "  - e.g.  by bayes inference, $p(x, y)=p(x|y)~p(y)=p(y|x)~p(x)$. If it's difficult to determine the nature of $p(x, y)$, while it is easy to sample from $p(y|x)$  and $p(x|y)$, then we can obtain a sequence of observations approximating $p(x, y)$ by sampling $p(y|x)$ and $p(x|y)$. \n",
    "  - Let's assume:\n",
    "    * $p(y|x)$ \\~ $N(x,\\sigma_1)$\n",
    "    * $p(x|y)$ \\~ $N(y,\\sigma_2)$ \n",
    "    * Now we need to have 100 $(x,y)$ samples so that we can understand the nature of $p(x,y)$\n",
    "  - The Gibbs sampler proceeds as follows:\n",
    "    1. set $x$ and $y$ to some initial starting values \n",
    "    2. Replace $x$ by a new value obtained by sampling $x|y$, then update $y$ by sampling from $y|x$, \n",
    "    3. Repeat step 2 until the estimates of parameters converge.\n",
    "- Outline of Gibbs Sampling in LDA\n",
    "  1. Go through each document and randomly assign each word in the document to one of the $K$ topics, i.e. $p(z_i=j~|~w_i, d)$, where $z_i=j$ denotes word $i$ is assigned to topic $j$. \n",
    "  2. Calculate the following\n",
    "     - $p(w_i~|~\\theta_j)$ (word-topic matrix): calculated as the count of each word being assigned to each topic. \n",
    "     - $p(\\theta_j~|~d)$ (document-topic matrix): the number of words assigned to each topic for each document \n",
    "  3. Update $p(z_i=j~|~w_i, d)$ using $p(w_i~|~\\theta_j)$ and $p(\\theta_j~|~d)$ as follows:\n",
    "     - For each document $d$, and each word $w_i$, reassign a new topic $j$ to $w_i$, where we choose topic $j$ is sampled from [1, 2, ..., $K$] with a probability ${\\propto}~ (w_i~|~\\theta_j) * p(\\theta_j~|~d)$ (conditional posterior distribution)\n",
    "     - i.e. Given $w_i$, $d$, $z_{-i}$ (topic assignment of all other words in $d$), $$p(z_i=j~|~w_i, z_{-i}, d)~{\\propto}~p(w_i~|~z_i=j, z_{-i}, d) * p(z_i=j~|~z_{-i}, d)~{\\propto}~(w_i~|~\\theta_j) * p(\\theta_j~|~d)$$ \n",
    "  4. Repeat steps 2-3 until $p(w_i~|~\\theta_j)$ and  $p(\\theta_j~|~d)$ converge.\n",
    "- For more implementation details, check http://ethen8181.github.io/machine-learning/clustering_old/topic_model/LDA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate Topic Model - Perplexity\n",
    "- For a single document $d$ with $N_d$ words $\\{w_1, w_2, ..., w_{N_d}\\}$, denoted as $\\textbf{W}_d$\n",
    "$$\n",
    "  perplexity(d)= exp({H(d)}),  \n",
    "  H(d) = - \\frac{ln (p(\\textbf{W}_d))}{N_d}  \n",
    "$$\n",
    "- $p(\\textbf{W}_d)$, the probability of seeing a document $d$, can be calculated based on:\n",
    "   - word distribution per topic, i.e. $p(w_i~|~\\theta_j$, and \n",
    "   - topic mixture, i.e. $p(\\theta_j~|~d)$\n",
    "- For a test set of D with M documents\n",
    "$$ perplexity(d)= exp({H(D)}), H(D) = - \\frac{\\sum_{d \\in D} {ln   (p(\\textbf{W}_d)})}{\\sum_{d \\in D}{N_d}} $$\n",
    "- Intutition: \n",
    "  - A lower perplexity score indicates better generalization performance\n",
    "  - Minimizing H(d) is equivalent to maximizing log likelihood\n",
    "- To evaluate a topic model, calcuate perplexity on **testing dataset** (i.e. evaluating how generaalized the model is)\n",
    "- Note: if you have some labeled data, you should also conduct **external evaluation**, i.e. \n",
    "  - map each topic to a labeled class, \n",
    "  - compute precision/recall from the labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiement with LDA\n",
    "- A few libraries available for LDA: gensim, lda, sklearn\n",
    "- We use sklearn here since it has a good text preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.1. Load 20 news group data\n",
    "import json\n",
    "from numpy.random import shuffle\n",
    "\n",
    "data=json.load(open('ydata.json','r'))\n",
    "\n",
    "# shuffle the data\n",
    "shuffle(data)\n",
    "\n",
    "text,label=zip(*data)\n",
    "text=list(text)\n",
    "label=list(label)\n",
    "\n",
    "print(text[0])\n",
    "print(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.2. Preprocessing - Create Term Frequency Matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# LDA can only use raw term counts for LDA \n",
    "tf_vectorizer = CountVectorizer(max_df=0.90, \\\n",
    "                min_df=50, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(text)\n",
    "\n",
    "# each feature is a word (bag of words)\n",
    "# get_feature_names() gives all words\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "print(tf_feature_names[0:10])\n",
    "print(tf.shape)\n",
    "\n",
    "# split dataset into train (90%) and test sets (10%)\n",
    "# the test sets will be used to evaluate proplexity of topic modeling\n",
    "X_train, X_test = train_test_split(\\\n",
    "                tf, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.3. Train LDA model\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "num_topics = 4\n",
    "\n",
    "# Run LDA. For details, check\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation.perplexity\n",
    "\n",
    "# max_iter control the number of iterations \n",
    "# evaluate_every determines how often the perplexity is calculated\n",
    "# n_jobs is the number of parallel threads\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, \\\n",
    "                                max_iter=10,verbose=1,\n",
    "                                evaluate_every=1, n_jobs=1,\n",
    "                                random_state=0).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.4. Check topic and word distribution per topic\n",
    "\n",
    "num_top_words=20\n",
    "\n",
    "# lda.components_ returns a KxN matrix\n",
    "# for word distribution in each topic.\n",
    "# Each row consists of \n",
    "# probability (counts) of each word in the feature space\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print (\"Topic %d:\" % (topic_idx))\n",
    "    # print out top 20 words per topic \n",
    "    words=[(tf_feature_names[i],topic[i]) for i in topic.argsort()[::-1][0:num_top_words]]\n",
    "    print(words)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import math\n",
    "\n",
    "num_top_words=50\n",
    "f, axarr = plt.subplots(2, 2, figsize=(8, 8));\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    # create a dataframe with two columns (word, weight) for each topic\n",
    "    \n",
    "    # create a word:count dictionary\n",
    "    f={tf_feature_names[i]:topic[i] for i in topic.argsort()[::-1][0:num_top_words]}\n",
    "    \n",
    "    # generate wordcloud in subplots\n",
    "    wordcloud = WordCloud(width=480, height=450, margin=0, background_color=\"black\");\n",
    "    _ = wordcloud.generate_from_frequencies(frequencies=f);\n",
    "    \n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].imshow(wordcloud, interpolation=\"bilinear\");\n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].set_title(\"Topic: \"+str(topic_idx));\n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.5. Assign documents to topic\n",
    "import numpy as np\n",
    "\n",
    "# Generate topic assignment of each document\n",
    "topic_assign=lda.transform(X_train)\n",
    "\n",
    "print(topic_assign[0:5])\n",
    "\n",
    "# set a probability threshold\n",
    "# the threshold determines precision/recall\n",
    "prob_threshold=0.25\n",
    "\n",
    "topics=np.copy(topic_assign)\n",
    "topics=np.where(topics>=prob_threshold, 1, 0)\n",
    "print(topics[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.6. Evaluate topic models by perplexity of test data\n",
    "\n",
    "perplexity=lda.perplexity(X_test)\n",
    "print(perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Find the number of topics ($K$)\n",
    "- There are no \"golden\" rules to find K.\n",
    "- Perplexity may be one way for you to find the number of topics\n",
    "    - Typically, the best number of topics should be around the **lowest perplexity**\n",
    "- However, in practice, a few factors need to be considered:\n",
    "  - It is usually difficult for human to understand or visulaize a big number of topics (I'd suggest K < 20)\n",
    "  - You may manually scan the data to figure out possible topics in the data, but these topics may not be correlated with the hidden structure discovered by LDA\n",
    "  - Usually, after LDA, we need manually inspect each discovered topic, merge or trim topics to get semantically coherent but distinguishable topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.7. How to find the best number of topics?\n",
    "# Vary variable num_topics, e.g. set it to 2, 3, 5, ...\n",
    "# For each value, train LDA model, calculate perplexity on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "result=[]\n",
    "for num_topics in range(2,25):\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, \\\n",
    "                                learning_method='online', \\\n",
    "                                max_iter=10,verbose=0, n_jobs=1,\n",
    "                                random_state=0).fit(X_train)\n",
    "    p=lda.perplexity(X_test)\n",
    "    result.append([num_topics,p])\n",
    "    print(num_topics, p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(result, columns=[\"K\", \"Perlexity\"]).plot.line(x='K',y=\"Perlexity\");\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "- NMF is similar to LDA with different mathematical underpinning. It decompose document-term matrix into the product of \n",
    "  - feature matrix (i.e. word distribution per topic) and \n",
    "  - weight matrix (i.e. topic mixture per document) \n",
    "- NMF is very efficient for small matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 6.1. NMF transformation\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=20, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(text)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(tfidf_feature_names[0:10])\n",
    "print(tfidf.shape)\n",
    "\n",
    "X_train, X_test = train_test_split(tfidf, test_size=0.1, random_state=0)\n",
    "\n",
    "no_topics = 4\n",
    "\n",
    "# Run NMF\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "# init: ‘nndsvd’: Nonnegative Double Singular Value Decomposition (NNDSVD) better for sparseness\n",
    "# alpha: regularization\n",
    "\n",
    "nmf = NMF(n_components=no_topics, \\\n",
    "          random_state=1, alpha=0.01, init='nndsvd').fit(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 6.2. Get topic words\n",
    "\n",
    "num_top_words=20\n",
    "\n",
    "# lda.components_ returns a KxN matrix\n",
    "# for word distribution in each topic.\n",
    "# Each row consists of \n",
    "# probability (counts) of each word in the feature space\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print (\"Topic %d:\" % (topic_idx))\n",
    "    # print out top 20 words per topic \n",
    "    words=[(tfidf_feature_names[i],topic[i]) \\\n",
    "           for i in topic.argsort()[::-1][0:num_top_words]]\n",
    "    print(words)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import math\n",
    "\n",
    "num_top_words=50\n",
    "f, axarr = plt.subplots(2, 2, figsize=(8, 8));\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    # create a dataframe with two columns (word, weight) for each topic\n",
    "    \n",
    "    # create a word:count dictionary\n",
    "    f={tfidf_feature_names[i]:topic[i] \\\n",
    "       for i in topic.argsort()[::-1][0:num_top_words]}\n",
    "    \n",
    "    # generate wordcloud in subplots\n",
    "    wordcloud = WordCloud(width=480, height=450, margin=0, background_color=\"black\");\n",
    "    _ = wordcloud.generate_from_frequencies(frequencies=f);\n",
    "    \n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].imshow(wordcloud, interpolation=\"bilinear\");\n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].set_title(\"Topic: \"+str(topic_idx));\n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 6.3. Assign document to topics\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "topic_assign=nmf.transform(X_train)\n",
    "\n",
    "print(topic_assign[0:5])\n",
    "\n",
    "topics=normalize(np.copy(topic_assign), axis=1, norm='l1')\n",
    "print(topics[0:5])\n",
    "\n",
    "prob_threshold=0.25\n",
    "\n",
    "topics=np.where(topics>prob_threshold, 1, 0)\n",
    "print(topics[0:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
