{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Deep Learning and Text Analytics II</center>\n",
    "\n",
    "References:\n",
    "- General introduction\n",
    "     - https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/\n",
    "     - http://neuralnetworksanddeeplearning.com\n",
    "     - http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/\n",
    "- Word vector:\n",
    "     - https://code.google.com/archive/p/word2vec/\n",
    "- Keras tutorial\n",
    "     - https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "- CNN\n",
    "     - http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agenda\n",
    "- Introduction to neural networks\n",
    "- Word/Document Vectors (vector representation of words/phrases/paragraphs)\n",
    "- Convolutional neural network (CNN)\n",
    "- Application of CNN in text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word2Vector (a.k.a word embedding) and Doc2Vector\n",
    "\n",
    "### 4.1. Word2Vector\n",
    " - Vector representation of words (i.e. word vectors) learned using neural network\n",
    "   - e.g. \"apple\" : [0.35, -0.2, 0.4, ...], 'mongo':  [0.32, -0.18, 0.5, ...]\n",
    "   - Interesting properties of word vectors:\n",
    "    * **Words with similar semantics have close word vectors**\n",
    "    * **Composition**: e.g. vector(\"woman\")+vector(\"king\")-vector('man') $\\approx$ vector(\"queen\")\n",
    " - Models:\n",
    "   - **CBOW** (Continuous Bag of Words): Predict a target word based on context\n",
    "     - e.g. the fox jumped over the lazy dog\n",
    "     - Assuming symmetric context with window size 3, this sentence can create training samples: \n",
    "       - ([-, fox], the) \n",
    "       - ([the, jumped], fox) \n",
    "       - ([fox, over], jumped)\n",
    "       - ([jumped, the], over) \n",
    "       - ...\n",
    "       \n",
    "       <img src=\"cbow.png\" width=\"30%\">\n",
    "       source: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "   - **Skip Gram**: predict context based on target words\n",
    "   \n",
    "        <img src=\"skip_gram.png\" width=\"30%\">\n",
    "        source: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "   - Nagtive Sampling: \n",
    "       - When training a neural network, for each sample,  all weights are adjusted slightly so that it predicts that training sample more accurately. \n",
    "       - CBOW or skip gram models have tremendous number of weights, all of which would be updated slightly by every one of billions of training samples!\n",
    "       - Negative sampling addresses this by having **each training sample only modify a small percentage of the weights, rather than all of them**. \n",
    "       - e.g. when training with sample ([fox, over], jumped), update output weights connected to \"jumped\" along with a small number of other \"negative words\" sampled randomly\n",
    "       - For details, check http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up interactive shell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>This is a little longer and more detailed than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Only Michelle Branch save this album!!!!All gu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A surprisingly good book, given its inherently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>This is a wonderful, quiet and relaxing CD tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The lights that I received are absolutely not ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      2  This is a little longer and more detailed than...\n",
       "1      1  Only Michelle Branch save this album!!!!All gu...\n",
       "2      2  A surprisingly good book, given its inherently...\n",
       "3      2  This is a wonderful, quiet and relaxing CD tha...\n",
       "4      1  The lights that I received are absolutely not ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'little', 'longer', 'and', 'more', 'detailed', 'than', 'the', 'first', 'two', 'books', 'in', 'the', 'series', 'however', 'have', 'enjoyed', 'each', 'new', 'aspect', 'of', 'the', 'exciting', 'fantasy', 'universe'], ['only', 'michelle', 'branch', 'save', 'this', 'album', 'all', 'guys', 'play', 'along', 'with', 'unenthusiastic', 'beat', 'even', 'karl']]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.1.1 Train your word vector\n",
    "\n",
    "import pandas as pd\n",
    "import nltk,string\n",
    "\n",
    "# Load data\n",
    "data=pd.read_csv('amazon_review_large.csv', header=None)\n",
    "data.columns=['label','text']\n",
    "data.head()\n",
    "\n",
    "# tokenize each document into a list of unigrams\n",
    "# strip punctuations and leading/trailing spaces from unigrams\n",
    "# only unigrams with 2 or more characters are taken\n",
    "sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "             for token in nltk.word_tokenize(doc.lower()) \\\n",
    "                 if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2]\\\n",
    "             for doc in data[\"text\"]]\n",
    "print(sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-15 16:40:55,322 : INFO : collecting all words and their counts\n",
      "2018-04-15 16:40:55,323 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-04-15 16:40:55,523 : INFO : PROGRESS: at sentence #10000, processed 712002 words, keeping 37008 word types\n",
      "2018-04-15 16:40:55,733 : INFO : collected 55313 word types from a corpus of 1424320 raw words and 20000 sentences\n",
      "2018-04-15 16:40:55,735 : INFO : Loading a fresh vocabulary\n",
      "2018-04-15 16:40:55,900 : INFO : min_count=5 retains 12134 unique words (21% of original 55313, drops 43179)\n",
      "2018-04-15 16:40:55,901 : INFO : min_count=5 leaves 1361936 word corpus (95% of original 1424320, drops 62384)\n",
      "2018-04-15 16:40:55,955 : INFO : deleting the raw counts dictionary of 55313 items\n",
      "2018-04-15 16:40:55,958 : INFO : sample=0.001 downsamples 57 most-common words\n",
      "2018-04-15 16:40:55,961 : INFO : downsampling leaves estimated 1015542 word corpus (74.6% of prior 1361936)\n",
      "2018-04-15 16:40:55,963 : INFO : estimated required memory for 12134 words and 200 dimensions: 25481400 bytes\n",
      "2018-04-15 16:40:56,019 : INFO : resetting layer weights\n",
      "2018-04-15 16:40:56,233 : INFO : training model with 4 workers on 12134 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-04-15 16:40:57,247 : INFO : PROGRESS: at 14.42% examples, 725576 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-15 16:40:58,255 : INFO : PROGRESS: at 30.64% examples, 771160 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-15 16:40:59,261 : INFO : PROGRESS: at 47.22% examples, 793667 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-15 16:41:00,273 : INFO : PROGRESS: at 63.58% examples, 800484 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-15 16:41:01,275 : INFO : PROGRESS: at 79.84% examples, 804880 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-15 16:41:02,290 : INFO : PROGRESS: at 96.21% examples, 807260 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-15 16:41:02,518 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-15 16:41:02,520 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-15 16:41:02,530 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-15 16:41:02,536 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-15 16:41:02,537 : INFO : training on 7121600 raw words (5077568 effective words) took 6.3s, 806155 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# Train your own word vectors using gensim\n",
    "\n",
    "# gensim.models is the package for word2vec\n",
    "# check https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# for detailed description\n",
    "\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# print out tracking information\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# min_count: words with total frequency lower than this are ignored\n",
    "# size: the dimension of word vector\n",
    "# window: context window, i.e. the maximum distance \n",
    "#         between the current and predicted word \n",
    "#         within a sentence (i.e. the length of ngrams)\n",
    "# workers: # of parallel threads in training\n",
    "# for other parameters, check https://radimrehurek.com/gensim/models/word2vec.html\n",
    "wv_model = word2vec.Word2Vec(sentences, min_count=5, size=200, window=5, workers=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-15 16:41:22,384 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to word 'sound'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('vocals', 0.752764105796814),\n",
       " ('production', 0.7524744272232056),\n",
       " ('band', 0.7387509942054749),\n",
       " ('tunes', 0.7347677946090698),\n",
       " ('music', 0.7317479848861694)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to word 'sound' but not relevant to 'film'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rock', 0.7866778373718262),\n",
       " ('pop', 0.7602622509002686),\n",
       " ('songs', 0.7421398758888245),\n",
       " ('lyrics', 0.735541820526123),\n",
       " ('dance', 0.7265161275863647)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'movie' and 'film':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9271679194747968"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'movie' and 'city':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05824728121002956"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word does not match with others in the list of ['sound', 'music', 'graphics', 'actor', 'book']:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for 'movie':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.69296825,  1.9410071 ,  1.1032392 ,  0.28273162,  0.42957693,\n",
       "       -0.9365348 , -1.0965693 , -0.80102354, -0.6073362 , -0.6036657 ,\n",
       "       -0.755955  , -0.64555955,  0.38533428,  1.26458   , -1.4919953 ,\n",
       "        0.6493107 ,  0.68263006, -1.2019557 , -1.2168735 , -0.66030353,\n",
       "       -0.0716041 , -0.18498282, -1.0847113 ,  1.4383963 ,  0.17555667,\n",
       "       -0.9925629 , -0.2793453 , -0.25657752, -0.96071786,  0.3448194 ,\n",
       "        0.09386399,  0.92940575,  0.97134227,  0.8171112 ,  1.8990242 ,\n",
       "       -1.7758207 , -0.08644257,  1.7223121 ,  0.74421877, -0.40162137,\n",
       "        0.0881858 ,  0.39953655,  1.6021348 , -0.02983055,  0.19915771,\n",
       "        0.85322255,  0.6185111 ,  2.2444851 , -0.2344643 , -0.22815944,\n",
       "        0.14900081,  0.86710435,  0.55936015,  0.0411598 , -0.52943635,\n",
       "       -1.3750321 ,  0.28646645,  1.0315421 ,  0.82511675,  0.99012333,\n",
       "        0.4037915 , -2.0691526 , -1.7561779 ,  1.7496054 , -0.20156628,\n",
       "       -0.21941772, -0.20687418,  1.2774737 ,  0.7427888 , -0.45050713,\n",
       "        1.0487546 ,  0.60010064, -1.7995406 ,  0.09851029,  0.22402643,\n",
       "       -0.8061096 , -0.6564914 , -0.04863024,  1.725711  ,  0.52783626,\n",
       "       -0.11694033, -0.30434024,  1.6001759 ,  1.6344302 , -0.8013479 ,\n",
       "        0.1941349 , -0.97013533, -0.61101997, -1.660868  ,  0.7439447 ,\n",
       "       -0.24947512, -1.647724  , -0.6105517 , -0.41366148, -0.2503721 ,\n",
       "        0.7667317 ,  0.30546173,  0.8648413 , -1.0055537 , -0.427204  ,\n",
       "       -0.19055514,  2.192093  ,  0.4843938 , -0.38400736, -0.28727153,\n",
       "        1.5830182 ,  0.4432583 ,  0.45714605, -1.4717276 , -0.10437132,\n",
       "       -0.72749966, -0.34431782, -0.4177683 ,  1.7407732 ,  0.7115139 ,\n",
       "       -0.673807  ,  0.71169996,  0.1872202 ,  0.57449156,  1.3437622 ,\n",
       "       -0.997486  , -1.760286  ,  0.50313395,  0.08474069, -0.19124329,\n",
       "        0.6775411 , -1.2635237 ,  1.2424015 , -0.23416713,  0.04669657,\n",
       "       -0.76996857, -1.0277189 , -1.2171777 ,  0.18684994, -0.76070523,\n",
       "        0.01481919,  0.43038455,  1.7244341 , -1.017619  ,  1.1175245 ,\n",
       "        1.2870588 , -0.437923  , -0.0888012 , -1.8919635 ,  0.78385395,\n",
       "        0.71505326, -1.904455  , -0.23726743, -1.6479205 ,  0.16748106,\n",
       "       -1.5855839 ,  0.1267612 , -0.15933232,  0.5209517 , -0.4066828 ,\n",
       "        1.1894155 ,  0.755511  , -0.02328787, -0.314415  , -0.16623151,\n",
       "       -1.1832601 ,  2.2551734 , -0.33610842,  2.0262506 , -1.3869194 ,\n",
       "       -0.17376061,  0.06005171,  0.02874165,  1.2780638 ,  1.0673143 ,\n",
       "       -0.7186635 , -0.75654083,  0.34380674,  1.5918981 , -0.24121843,\n",
       "       -1.0339708 , -0.25308198,  0.0362062 , -0.2842836 ,  0.81480795,\n",
       "       -0.26562393, -0.04321614,  1.433164  , -0.5627058 , -1.5205634 ,\n",
       "       -0.67785144, -1.5254925 , -0.1702559 , -0.8525156 , -1.5514945 ,\n",
       "        0.4511479 , -0.75758773, -1.4266689 ,  1.4838157 ,  0.20785517,\n",
       "       -0.35600024,  0.4279215 , -0.14102909, -0.7149007 , -1.381657  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test word2vec model\n",
    "\n",
    "print(\"Top 5 words similar to word 'sound'\")\n",
    "wv_model.wv.most_similar('sound', topn=5)\n",
    "\n",
    "print(\"Top 5 words similar to word 'sound' but not relevant to 'film'\")\n",
    "wv_model.wv.most_similar(positive=['sound','music'], negative=['film'], topn=5)\n",
    "\n",
    "print(\"Similarity between 'movie' and 'film':\")\n",
    "wv_model.wv.similarity('movie','film') \n",
    "\n",
    "print(\"Similarity between 'movie' and 'city':\")\n",
    "wv_model.wv.similarity('movie','city') \n",
    "\n",
    "print(\"Word does not match with others in the list of \\\n",
    "['sound', 'music', 'graphics', 'actor', 'book']:\")\n",
    "wv_model.wv.doesnt_match([\"sound\", \"music\", \"graphics\", \"actor\", \"book\"])\n",
    "\n",
    "print(\"Word vector for 'movie':\")\n",
    "wv_model.wv['movie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Pretrained Word Vectors\n",
    "- Google published pre-trained 300-dimensional vectors for 3 million words and phrases that were trained on Google News dataset (about 100 billion words)(https://code.google.com/archive/p/word2vec/)\n",
    "- GloVe (Global Vectors for Word Representation): Pretained word vectors from different data sources provided by Standford https://nlp.stanford.edu/projects/glove/\n",
    "- FastText by Facebook https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-15 17:44:58,810 : INFO : loading projection weights from /Users/xiadylan/Downloads/GoogleNews-vectors-negative300.bin\n",
      "2018-04-15 17:46:13,643 : INFO : loaded (3000000, 300) matrix from /Users/xiadylan/Downloads/GoogleNews-vectors-negative300.bin\n",
      "2018-04-15 17:46:13,657 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('communications', 0.7521636486053467),\n",
       " ('Mahir_Yassin_director', 0.6448125839233398),\n",
       " ('communcation', 0.6222150921821594),\n",
       " ('interpersonal_communication', 0.6083678007125854),\n",
       " ('intercommunication', 0.602850079536438),\n",
       " ('Communication', 0.5903989672660828),\n",
       " ('communicating', 0.5881104469299316),\n",
       " ('communicate', 0.5804769992828369),\n",
       " ('communi_cation', 0.5486894249916077),\n",
       " ('interaction', 0.5432522892951965),\n",
       " ('communciation', 0.5343629121780396),\n",
       " ('communicative', 0.5312253832817078),\n",
       " ('com_munication', 0.5273781418800354),\n",
       " ('comunication', 0.5263152718544006),\n",
       " ('REDWOOD_RAMBLERS_TOASTMASTERS', 0.5249114036560059),\n",
       " ('communicators', 0.5175506472587585),\n",
       " ('Excellent_interpersonal', 0.5057379603385925),\n",
       " ('coordination', 0.5033478736877441),\n",
       " ('connectivity', 0.49323806166648865),\n",
       " ('intercommunications', 0.49195945262908936)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 4.2.1: Use pretrained word vectors\n",
    "\n",
    "# download the bin file for pretrained word vectors\n",
    "# from above links, e.g. https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "# Warning: the bin file is very big (over 2G)\n",
    "# You need a powerful machine to load it\n",
    "\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/xiadylan/Downloads/GoogleNews-vectors-negative300.bin', binary=True) \n",
    "\n",
    "#model.wv.most_similar(positive=['women','king'], negative='man')\n",
    "model.wv.most_similar('communication',topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('leader', 0.5659863948822021),\n",
       " ('Army_censured_Kensinger', 0.5293898582458496),\n",
       " ('Leadership', 0.5282222032546997),\n",
       " ('Targacept_leverages', 0.5193353891372681),\n",
       " ('leaderships', 0.5047478675842285),\n",
       " ('NNSA_Bodman', 0.503797173500061),\n",
       " ('leaders', 0.5006183981895447),\n",
       " ('chairmanship', 0.4903269410133362),\n",
       " ('organizational_structure', 0.48777055740356445),\n",
       " ('SIGAR_failing', 0.47965937852859497),\n",
       " ('followship', 0.47346433997154236),\n",
       " ('managerial_competence', 0.4674581289291382),\n",
       " ('Comrade_Mbeki', 0.4670478105545044),\n",
       " ('organizational', 0.4669966995716095),\n",
       " ('leadeship', 0.46600478887557983),\n",
       " ('Mr._Schimkaitis', 0.46522200107574463),\n",
       " ('SUSS_maintains', 0.4648105800151825),\n",
       " ('leaderhip', 0.4634861946105957),\n",
       " ('stewardship', 0.46299847960472107),\n",
       " ('competence', 0.4589086174964905),\n",
       " ('acumen', 0.4570310711860657),\n",
       " ('succeed', 0.4517122805118561),\n",
       " ('unquestionable_integrity', 0.4470038414001465),\n",
       " ('commitment', 0.4369075298309326),\n",
       " ('guru_Master_Shifu', 0.43597912788391113),\n",
       " ('helm', 0.43324869871139526),\n",
       " ('visionary', 0.43213289976119995),\n",
       " ('skilled_communicator', 0.430952250957489),\n",
       " ('creativity_innovativeness', 0.4304131269454956),\n",
       " ('managerial_acumen', 0.42587584257125854),\n",
       " ('deputy_Aslambek_Vadalov', 0.42350608110427856),\n",
       " ('reins', 0.42261621356010437),\n",
       " ('president', 0.42046499252319336),\n",
       " ('entrepreneurial_mindset', 0.41943904757499695),\n",
       " ('governance', 0.4188893437385559),\n",
       " ('Albert_A._Vicere', 0.418692409992218),\n",
       " ('directorship', 0.41815185546875),\n",
       " ('position', 0.4178329408168793),\n",
       " ('principled', 0.4177905321121216),\n",
       " ('incisive_intellect', 0.41624915599823),\n",
       " ('foresightedness', 0.41528379917144775),\n",
       " ('boardsmanship', 0.4146648049354553),\n",
       " ('Randy_Littleson_vice', 0.41387179493904114),\n",
       " ('captainship', 0.4135456681251526),\n",
       " ('Bryan_Derreberry', 0.4119946360588074),\n",
       " ('Chairmanship', 0.4115881025791168),\n",
       " ('pesident', 0.4107567071914673),\n",
       " ('foresight', 0.40727075934410095),\n",
       " ('vision', 0.40705323219299316),\n",
       " ('commitmentto', 0.4069724977016449)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"leadership\",topn=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Sentence/Paragraph/Document Vectors\n",
    "- So far we learned vector representation of words\n",
    "- A lot of times, our samples are sentences, paragraphs, or documents\n",
    "- How to create vector representations of sentences, paragraphs, or documents?\n",
    "  - Weighted average of word vectors (however, word order is lost as \"bag of words\")\n",
    "  - Concatenation of word vectors (large space)\n",
    "  - ??\n",
    "- Paragraph Vector: A distributed memory model (PV-DM)\n",
    "   - Word vectors are shared across paragraphs\n",
    "   - The paragraph vector is shared across all contexts generated from the same paragraph but not across paragraphs\n",
    "   - **Both pragraph vectors and word vectors** are returned\n",
    "   - Paragraph vectors can be used for document retrival or as features for classification or clustering\n",
    "  <img src=\"doc2vec.png\" width=\"50%\">\n",
    "   Source: Le Q. and Mikolov, T. Distributed Representations of Sentences and Documents https://arxiv.org/pdf/1405.4053v2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 4.3.1 Train your word vector\n",
    "\n",
    "# We have tokenized sentences\n",
    "# Label each sentence with a unique tag\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "docs=[TaggedDocument(sentences[i], [str(i)]) for i in range(len(sentences)) ]\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# package for doc2vec\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "# for more parameters, check\n",
    "# https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "# initialize the model without documents\n",
    "# distributed memory model is used (dm=1)\n",
    "model = doc2vec.Doc2Vec(dm=1, min_count=5, window=5, size=200, workers=4)\n",
    "\n",
    "# build the vocabulary using the documents\n",
    "model.build_vocab(docs) \n",
    "\n",
    "# train the model in 20 epoches\n",
    "# You may need to incease epoches\n",
    "for epoch in range(30):\n",
    "    # shuffle the documents in each epoch\n",
    "    shuffle(docs)\n",
    "    # in each epoch, all samples are used\n",
    "    model.train(docs, total_examples=len(docs), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspect paragraph vectors and word vectors\n",
    "\n",
    "# the pragraph vector of the first document\n",
    "model.docvecs['0']\n",
    "\n",
    "# the word vector of 'movie'\n",
    "model.wv['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check word similarity\n",
    "\n",
    "print(\"Top 5 words similar to word 'sound'\")\n",
    "model.wv.most_similar('sound', topn=5)\n",
    "\n",
    "print(\"Top 5 words similar to word 'sound' but not relevant to 'film'\")\n",
    "model.wv.most_similar(positive=['sound','music'], negative=['film'], topn=5)\n",
    "\n",
    "print(\"Similarity between 'movie' and 'film':\")\n",
    "model.wv.similarity('movie','film') \n",
    "\n",
    "print(\"Similarity between 'movie' and 'city':\")\n",
    "model.wv.similarity('movie','city') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspect document similarity\n",
    "\n",
    "model.docvecs.most_similar('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convolutional Neural Networks (CNN)\n",
    "References (**highly recommended**): \n",
    "- http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "- https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- CNNs are widely used in Computer Vision\n",
    "- CNNs were responsible for **major breakthroughs** in **Image Recognition** and are the core of most Computer Vision systems including automated photo tagging, self-driving cars\n",
    "- Recently, CNNs have been applied in NLP and achieved good performance.\n",
    "<img src='cnn.png' width='90%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Convolution\n",
    "- Convolution is the technique to **extract distinguishing features** from feature spaces\n",
    "- Example: feature detection from image pixels\n",
    "  - Feature space: a matrix of pixels of 0 (black) or 1 (white)\n",
    "  - **Filter/kernal/feature Detector**: a function applied to every fixed subset of the feature matrix\n",
    "    - e.g. 3x3 filter (a 3x3 matrix $\\begin{vmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{vmatrix}$ ) slides through every area of the matrix sequentially, multiplies its values element-wise with the original matrix, then sum them up\n",
    "    - e.g. a filter (e.g. $\\begin{vmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & -4 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{vmatrix}$ ) to take difference between a pixel and its neighbors --> detect edges\n",
    "    <img src='convolution.png' width=\"60%\">\n",
    "- Typically, a larger number of filters in different sizes will be used\n",
    "- Configuration of filters\n",
    "  - filter size ($h \\text{x} w$)\n",
    "  - stride size (how much to shift a filter in each step) ($s$)\n",
    "  - number of filters (depth) ($d$)\n",
    "- Questions: \n",
    "  - With 5x5 feature space, afte apply a filter of size 3x3 with stride size 2, what will be the size of the result? \n",
    "  - Formula to calculate the size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Pooling Layer\n",
    "- Pooling layers are typically applied after the convolutional layers. \n",
    "- A pooling layer subsamples its input. \n",
    "- The most common way to do pooling is to apply a **max** operation to the result of each filter (a.k.a 1-max pooling).\n",
    "  - e.g. for the example below, by 1-max pooling, we get 8.\n",
    "  - If 100 filters have been used, then we get 100 numbers\n",
    "- Pooling can be applied over a window (e.g. 2x2)\n",
    "<img src='max_pooling.png' width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. What are CNNs\n",
    "- CNNs consists of several layers of convolutions with nonlinear activation functions like ReLU or tanh \n",
    "\n",
    "<img src='cnn.png' width='70%'>\n",
    "\n",
    "- A CNN typically contains:\n",
    "  - A **convolution layer** (not dense layer) connected to the input layer\n",
    "      - Each convolution layer applies different filters. \n",
    "      - Typically hundreds or thousands filters used. \n",
    "      - The results of filters are concatenated.\n",
    "  - A **pooling layer** is used to subsample the result of convolution layer\n",
    "  - There may be multiple layers of convolution and pooling combined. E.g. image detection\n",
    "    - 1st layer: detect edges\n",
    "    - 2nd layer: detect shape, e.g. round, square\n",
    "    - 3rd layer: wheels, doors etc.\n",
    "  - Then each result out of convolution-pooling is connected to a neuron in the output (local connections). Such  results results are high-level features used by classification algorithms. \n",
    "- During the training phase, a CNN **automatically learns the values of its filters based on the task you want to perform**. \n",
    "- Powerful capabilities of CNN:\n",
    "  - **Location Invariance**: CNN extracts distinguishing features by convolution-pooling and it does not care where these features are. So images can still be recognized after rotation and scaling.\n",
    "  - **Compositionality**: Each filter composes a local patch of lower-level features into higher-level representation. E.g., detect edges from pixels, shapes from edges, and more complex objects from shapes. \n",
    "- If you're interested in how CNNs are used in image recognition, follow the classical <a href=\"https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\">MNIST handwritten digit recognition tutorial</a>\n",
    "- Play with it! http://scs.ryerson.ca/~aharley/vis/conv/flat.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Application of CNN in Text Classification\n",
    "- Assume $m$ samples, each of which is a sentence with $n$ words (short sentences can be padded)\n",
    "- **Embedding**: In each sentence, each word can be represented as its word vector of dimension $d$ (pretrained or to be trained)\n",
    "- **Convolution**: Apply filters to n-grams of different lengths (e.g. unigram, bigrams, ...). \n",
    "   - E.g. A filter can slide through every 2 words (bigram)\n",
    "   - So, the filter size (i.e. region size) can be $1\\text{x}d$ (unigram), $2\\text{x}d$ (bigram), $3\\text{x}d$ (trigram), ...\n",
    "- At pooling layer, 1-max pooling is applied to the result of each filter. Then all results after pooling are concatenated as the input to the output layer\n",
    "  - This is equivalent to select words or phrases that are **discriminative** with regard to the classification goal\n",
    "\n",
    "<img src='cnn_text_classification.png' width='70%'>\n",
    "\n",
    "*Illustration of a Convolutional Neural Network (CNN) architecture for sentence classification. Here we depict three filter region sizes: 2, 3 and 4, each of which has 2 filters. Every filter performs convolution on the sentence matrix and generates (variable-length) feature maps. Then 1-max pooling is performed over each map, i.e., the largest number from each feature map is recorded. Thus a univariate feature vector is generated from all six maps, and these 6 features are concatenated to form a feature vector for the penultimate layer. The final softmax layer then receives this feature vector as input and uses it to classify the sentence; here we assume binary classification and hence depict two possible output states. Source: Zhang, Y., & Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification.*\n",
    "\n",
    "- Questions:\n",
    "  - How many parameters in total in the convolution layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. How to deal with overfitting - Regularization & Dropout\n",
    "- Deep neural nets with a large number of parameters can be easily suffer from overfitting \n",
    "- Typical approaches to overcome overfitting\n",
    "    - Regularization\n",
    "    - Dropout (which is also a kind of regularization technique)\n",
    "- What is dropout?\n",
    "  - During training, randomly remove units in the hidden layer from the network. Update parameters as normal, leaving dropped-out units unchanged\n",
    "  - No dropout during testing \n",
    "  - Typically, each hidden unit is set to 0 with probability 0.5\n",
    "  <img src='dropout.png' width='60%'>\n",
    "  https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    " \n",
    "- Why dropout?\n",
    "  - Hidden units cannot co-adapt with other units since a unit may not always be present \n",
    "  - Sample data usually come with noise. Dropout constrains network adaptation to the data at training time\n",
    "  - After training, only very useful neurons are kept (have high weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Example: Use CNN for Sentiment Analysis (Single-Label Classification)\n",
    "- Dataset: IMDB review\n",
    "- 25,000 movie reviews, positive or negative \n",
    "- Benchmark performance is 80-90% with CNN (https://arxiv.org/abs/1408.5882)\n",
    "- We're going to create a CNN with the following:\n",
    "  - Word embedding trained as part of CNN\n",
    "  - filters in 3 sizes:\n",
    "      - unigram (Conv1D, kernel_size=1)\n",
    "      - bigram (Conv1D, kernel_size=2)\n",
    "      - trigram (Conv1D, kernel_size=3)\n",
    "  - Maxpooling for each convolution layer\n",
    "  - Dropout\n",
    "  <img src=\"cnn_model.png\" width='60%'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 5.1: Load data\n",
    "\n",
    "import pandas as pd\n",
    "import nltk,string\n",
    "from gensim import corpora\n",
    "\n",
    "data=pd.read_csv(\"imdb_reviews.csv\", header=0, delimiter=\"\\t\")\n",
    "data.head()\n",
    "len(data)\n",
    "\n",
    "# if your computer does not have enough resource\n",
    "# reduce the dataset\n",
    "data=data.loc[0:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  17   31   13  533  166  190   32    1  560   17 6924  216  604 2370\n",
      "    5   25  223  154    1  940  650  134    2   49  307    1    2  307\n",
      " 9875  167  301   12   42  187    5   80    3  730 2796   81   13  234\n",
      "   37   12  201   15   65  656   10    1 3941   42    5  301   94   55\n",
      "   60  334  713   29    6 2284   41 1391 9875    6  175 4707  175  807\n",
      "   21   61   12  373  166    5   66   32    1  431   53   11   15 1792\n",
      "  715   48    4   11   44 1332 3768   43  553  934    1 3769    2   79\n",
      "    1  548  756    4 1760   26   87    8    9    7    8    9    7 2249\n",
      " 1256   20    4  254   13    6   31   43  507 2007   38  842   24 2335\n",
      "   39 6924   10  563   91   24   26  166    5  839   13    2  163   11\n",
      "  396   48  199  725 6924   34   18    5    1  230    4   13   19   20\n",
      " 6924    2   86    4   25  448   62  132   14   29   93   11   18    1\n",
      "  448   61   46  279    6   65  339    4   88    8    9    7    8    9\n",
      "    7    1  843  807   21  227   53   11  432  516    6   63   22   18\n",
      "  992  241   41   38    1 3770 1607  714    2  805    6 1206   16    3\n",
      "   31  901 1437 1649  135   29  534 6924  345   38   87    6  727   72\n",
      "   84 6924   25 2336  805  109   14   29  487   77    5  124   11    6\n",
      "   29   37    6 1760  499   38   12  301   29   42 4148  223    8    9\n",
      "    7    8    9    7  669    4  656  182   10   13   39 6924 1450   81\n",
      "    3  501    2    3 2586    2    1  235 1861 3111  714   79    1  162\n",
      "  207   27   68    1 4824    4    3 5096   53   11  354    5 1417    1\n",
      "   87  714   16  629  935  839  693   17   30  561  399  579    3  235\n",
      "  719    4   97 3501    3 1340  820  137    8    9    7    8    9    7\n",
      " 1232  351   13   19    6   18   77   37   39 6924   22   30  598   41\n",
      "  159   61   12  103    6   86   77   46   23   91  731  244   11  126\n",
      "  358    2  202  122    3 7899  756    2 3619 2008   10   13   19    6\n",
      "    3  266  507 2007    6  379   30    4    1   86 1019   77  125    5\n",
      " 1556   13 1199   20    6   29 2284   71   17   31    1  710  216  536\n",
      "   13  874 6925   71   12   92  124   84   77   69   28  274  525 4964\n",
      " 4149   12  124   13   18    3  197   29    6  349   34  631  339   20\n",
      "  391  234   41   30    4    1   86   12  441   29    6   23    1 1530\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.2 Prepocessing data: Tokenize, pad sentences\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "# set the maximum number of words to be used\n",
    "MAX_NB_WORDS=10000\n",
    "\n",
    "\n",
    "# set sentence/document length\n",
    "MAX_DOC_LEN=500\n",
    "\n",
    "\n",
    "# get a Keras tokenizer\n",
    "# https://keras.io/preprocessing/text/\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data[\"review\"])\n",
    "\n",
    "\n",
    "# convert each document to a list of word index as a sequence\n",
    "sequences = tokenizer.texts_to_sequences(data[\"review\"])\n",
    "\n",
    "\n",
    "# pad all sequences into the same length \n",
    "# if a sentence is longer than maxlen, pad it in the right\n",
    "# if a sentence is shorter than maxlen, truncate it in the right\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_DOC_LEN, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "print(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13067"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the mapping between word and its index\n",
    "tokenizer.word_index['film']\n",
    "\n",
    "# get the count of each word\n",
    "tokenizer.word_counts['film']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "                        padded_sequences, data['sentiment'],\\\n",
    "                        test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.3: Create CNN model\n",
    "\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Dropout, Activation, Input, Flatten, Concatenate\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "# The dimension for embedding\n",
    "EMBEDDING_DIM=100\n",
    "\n",
    "# define input layer, where a sentence represented as\n",
    "# 1 dimension array with integers\n",
    "main_input = Input(shape=(MAX_DOC_LEN,), dtype='int32', name='main_input')\n",
    "\n",
    "# define the embedding layer\n",
    "# input_dim is the size of all words +1\n",
    "# where 1 is for the padding symbol\n",
    "# output_dim is the word vector dimension\n",
    "# input_length is the max. length of a document\n",
    "# input to embedding layer is the \"main_input\" layer\n",
    "embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, output_dim=EMBEDDING_DIM, input_length=MAX_DOC_LEN,name='embedding')(main_input)\n",
    "\n",
    "\n",
    "# define 1D convolution layer\n",
    "# 64 filters are used\n",
    "# a filter slides through each word (kernel_size=1)\n",
    "# input to this layer is the embedding layer\n",
    "conv1d_1= Conv1D(filters=64, kernel_size=1, name='conv_unigram',activation='relu')(embed_1)\n",
    "\n",
    "\n",
    "# define a 1-dimension MaxPooling \n",
    "# to take the output of the previous convolution layer\n",
    "# the convolution layer produce \n",
    "# MAX_DOC_LEN-1+1 values as ouput (???)\n",
    "pool_1 = MaxPooling1D(MAX_DOC_LEN-1+1, name='pool_unigram')(conv1d_1)\n",
    "\n",
    "\n",
    "# The pooling layer creates output \n",
    "# in the size of (# of sample, 1, 64)  \n",
    "# remove one dimension since the size is 1\n",
    "flat_1 = Flatten(name='flat_unigram')(pool_1)\n",
    "\n",
    "# following the same logic to define \n",
    "# filters for bigram\n",
    "conv1d_2= Conv1D(filters=64, kernel_size=2, name='conv_bigram',activation='relu')(embed_1)\n",
    "\n",
    "pool_2 = MaxPooling1D(MAX_DOC_LEN-2+1, name='pool_bigram')(conv1d_2)\n",
    "\n",
    "flat_2 = Flatten(name='flat_bigram')(pool_2)\n",
    "\n",
    "# filters for trigram\n",
    "conv1d_3= Conv1D(filters=64, kernel_size=3, name='conv_trigram',activation='relu')(embed_1)\n",
    "pool_3 = MaxPooling1D(MAX_DOC_LEN-3+1, name='pool_trigram')(conv1d_3)\n",
    "flat_3 = Flatten(name='flat_trigram')(pool_3)\n",
    "\n",
    "# Concatenate flattened output\n",
    "z=Concatenate(name='concate')([flat_1, flat_2, flat_3])\n",
    "\n",
    "# Create a dropout layer\n",
    "# In each iteration only 50% units are turned on\n",
    "drop_1=Dropout(rate=0.5, name='dropout')(z)\n",
    "\n",
    "# Create a dense layer\n",
    "dense_1 = Dense(192, activation='relu', name='dense')(drop_1)\n",
    "# Create the output layer\n",
    "preds = Dense(1, activation='sigmoid', name='output')(dense_1)\n",
    "\n",
    "# create the model with input layer\n",
    "# and the output layer\n",
    "model = Model(inputs=main_input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding (Embedding)            (None, 500, 100)      1000100     main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv_unigram (Conv1D)            (None, 500, 64)       6464        embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_bigram (Conv1D)             (None, 499, 64)       12864       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_trigram (Conv1D)            (None, 498, 64)       19264       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "pool_unigram (MaxPooling1D)      (None, 1, 64)         0           conv_unigram[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "pool_bigram (MaxPooling1D)       (None, 1, 64)         0           conv_bigram[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "pool_trigram (MaxPooling1D)      (None, 1, 64)         0           conv_trigram[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flat_unigram (Flatten)           (None, 64)            0           pool_unigram[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flat_bigram (Flatten)            (None, 64)            0           pool_bigram[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flat_trigram (Flatten)           (None, 64)            0           pool_trigram[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concate (Concatenate)            (None, 192)           0           flat_unigram[0][0]               \n",
      "                                                                   flat_bigram[0][0]                \n",
      "                                                                   flat_trigram[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 192)           0           concate[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense (Dense)                    (None, 192)           37056       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 1)             193         dense[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,075,941\n",
      "Trainable params: 1,075,941\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.4: Show model configuration\n",
    "\n",
    "model.summary()\n",
    "#model.get_config()\n",
    "#model.get_weights()\n",
    "#from keras.utils import plot_model\n",
    "#plot_model(model, to_file='cnn_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.4: Compile the model\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", \\\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 2401 samples\n",
      "Epoch 1/10\n",
      "30s - loss: 0.6572 - acc: 0.6039 - val_loss: 0.5015 - val_acc: 0.8113\n",
      "Epoch 2/10\n",
      "29s - loss: 0.4205 - acc: 0.8050 - val_loss: 0.3584 - val_acc: 0.8509\n",
      "Epoch 3/10\n",
      "33s - loss: 0.2829 - acc: 0.8809 - val_loss: 0.3408 - val_acc: 0.8617\n",
      "Epoch 4/10\n",
      "33s - loss: 0.1878 - acc: 0.9289 - val_loss: 0.3445 - val_acc: 0.8609\n",
      "Epoch 5/10\n",
      "32s - loss: 0.1198 - acc: 0.9550 - val_loss: 0.3895 - val_acc: 0.8551\n",
      "Epoch 6/10\n",
      "30s - loss: 0.0662 - acc: 0.9782 - val_loss: 0.4255 - val_acc: 0.8526\n",
      "Epoch 7/10\n",
      "30s - loss: 0.0498 - acc: 0.9843 - val_loss: 0.4657 - val_acc: 0.8555\n",
      "Epoch 8/10\n",
      "30s - loss: 0.0349 - acc: 0.9900 - val_loss: 0.5245 - val_acc: 0.8496\n",
      "Epoch 9/10\n",
      "30s - loss: 0.0289 - acc: 0.9913 - val_loss: 0.5407 - val_acc: 0.8467\n",
      "Epoch 10/10\n",
      "30s - loss: 0.0214 - acc: 0.9938 - val_loss: 0.5819 - val_acc: 0.8438\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.5: Fit the model\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHES = 10\n",
    "\n",
    "# fit the model and save fitting history to \"training\"\n",
    "training=model.fit(X_train, y_train, \\\n",
    "                   batch_size=BATCH_SIZE, \\\n",
    "                   epochs=NUM_EPOCHES,\\\n",
    "                   validation_data=[X_test, y_test], \\\n",
    "                   verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       train_acc  train_loss   val_acc  val_loss\n",
      "epoch                                           \n",
      "0       0.603929    0.657194  0.811329  0.501535\n",
      "1       0.805000    0.420511  0.850895  0.358397\n",
      "2       0.880893    0.282917  0.861724  0.340756\n",
      "3       0.928929    0.187809  0.860891  0.344496\n",
      "4       0.955000    0.119822  0.855060  0.389527\n",
      "5       0.978214    0.066183  0.852561  0.425547\n",
      "6       0.984286    0.049786  0.855477  0.465693\n",
      "7       0.990000    0.034873  0.849646  0.524550\n",
      "8       0.991250    0.028936  0.846731  0.540742\n",
      "9       0.993750    0.021364  0.843815  0.581872\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAADTCAYAAAC7kdtbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nomOSnCMiIjrmNxzGL4EgkIyfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c3555d0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exercise 5.6. Investigate the training process\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# plot a figure with size 20x8\n",
    "\n",
    "# the fitting history is saved as dictionary\n",
    "# covert the dictionary to dataframe\n",
    "df=pd.DataFrame.from_dict(training.history)\n",
    "df.columns=[\"train_acc\", \"train_loss\", \\\n",
    "            \"val_acc\", \"val_loss\"]\n",
    "df.index.name='epoch'\n",
    "print(df)\n",
    "\n",
    "# plot training history\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,3));\n",
    "\n",
    "df[[\"train_acc\", \"val_acc\"]].plot(ax=axes[0]);\n",
    "df[[\"train_loss\", \"val_loss\"]].plot(ax=axes[1]);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from training history:\n",
    "- As training goes on, training accuracy/loss gets always better\n",
    "- Testing accuracy/loss gets better at the beginning, the gets worse\n",
    "- This indicates that model is **overfitted** and cannot be generalized after certain point\n",
    "- Thus, we should **stop training the model when testing accuracy/loss gets worse**. \n",
    "- This analysis can be used to determine hyperparameter **NUM_EPOCHES**\n",
    "- Fortunately, this can be done automatically by **\"Early Stopping\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 2401 samples\n",
      "Epoch 1/10\n",
      "Epoch 00000: val_acc improved from -inf to 0.84923, saving model to best_model\n",
      "30s - loss: 0.0234 - acc: 0.9916 - val_loss: 0.6665 - val_acc: 0.8492\n",
      "Epoch 2/10\n",
      "Epoch 00001: val_acc did not improve\n",
      "29s - loss: 0.0135 - acc: 0.9950 - val_loss: 0.7354 - val_acc: 0.8471\n",
      "Epoch 00001: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c3250bcc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 5.6: Use early stopping to find the best model\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# the file path to save best model\n",
    "BEST_MODEL_FILEPATH=\"best_model\"\n",
    "\n",
    "# define early stopping based on validation loss\n",
    "# if validation loss is not improved in \n",
    "# an iteration compared with the previous one, \n",
    "# stop training (i.e. patience=0). \n",
    "# mode='min' indicate the loss needs to decrease \n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "\n",
    "\n",
    "# define checkpoint to save best model\n",
    "# which has max. validation acc\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_acc', verbose=2, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "\n",
    "# compile model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "# fit the model with earlystopping and checkpoint\n",
    "# as callbacks (functions that are executed as soon as \n",
    "# an asynchronous thread is completed)\n",
    "model.fit(X_train, y_train, \\\n",
    "          batch_size=BATCH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\n",
    "          validation_data=[X_test, y_test],\\\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9892455e-01]\n",
      " [9.8414892e-01]\n",
      " [9.9999988e-01]\n",
      " [3.9199274e-03]\n",
      " [1.3991306e-06]]\n",
      "acc: 84.92%\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.7: Load the best model\n",
    "\n",
    "# load the model using the save file\n",
    "model.load_weights(\"best_model\")\n",
    "\n",
    "# predict\n",
    "pred=model.predict(X_test)\n",
    "print(pred[0:5])\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.8: Put Everything as a function\n",
    "\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "              \n",
    "def cnn_model(FILTER_SIZES, \\\n",
    "              # filter sizes as a list\n",
    "              MAX_NB_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_DOC_LEN, \\\n",
    "              # max words in a doc\n",
    "              EMBEDDING_DIM=200, \\\n",
    "              # word vector dimension\n",
    "              NUM_FILTERS=64, \\\n",
    "              # number of filters for all size\n",
    "              DROP_OUT=0.5, \\\n",
    "              # dropout rate\n",
    "              NUM_OUTPUT_UNITS=1, \\\n",
    "              # number of output units\n",
    "              NUM_DENSE_UNITS=100,\\\n",
    "              # number of units in dense layer\n",
    "              PRETRAINED_WORD_VECTOR=None,\\\n",
    "              # Whether to use pretrained word vectors\n",
    "              LAM=0.0):            \n",
    "              # regularization coefficient\n",
    "    \n",
    "    main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                       dtype='int32', name='main_input')\n",
    "    \n",
    "    if PRETRAINED_WORD_VECTOR is not None:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                        trainable=False,\\\n",
    "                        name='embedding')(main_input)\n",
    "    else:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        name='embedding')(main_input)\n",
    "    # add convolution-pooling-flat block\n",
    "    conv_blocks = []\n",
    "    for f in FILTER_SIZES:\n",
    "        conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                      activation='relu', name='conv_'+str(f))(embed_1)\n",
    "        conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "        conv = Flatten(name='flat_'+str(f))(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    \n",
    "    if len(conv_blocks)>1:\n",
    "        z=Concatenate(name='concate')(conv_blocks)\n",
    "    else:\n",
    "        z=conv_blocks[0]\n",
    "        \n",
    "    drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "    dense = Dense(NUM_DENSE_UNITS, activation='relu',\\\n",
    "                    kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "    preds = Dense(NUM_OUTPUT_UNITS, activation='sigmoid', name='output')(dense)\n",
    "    model = Model(inputs=main_input, outputs=preds)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7. Use CNN for multi-label classification\n",
    "- In multi-label classification, a document can be classified into multiple classes\n",
    "- We can use **multiple ouput units**, each responsible for predicating one class\n",
    "- For multi-label classification ($K$ classes), do the following:\n",
    "    1. Represent the labels as **indication matrix**\n",
    "        - e.g. three classes ['econ','biz','tech'] in total, \n",
    "        - sample 1: 'eco' only -> [1, 0, 0]\n",
    "        - sample 2: ['eco','biz'] ->[1, 1, 0]\n",
    "    2. Accordingly, **set output layer to have K output units**\n",
    "        - each responsible for one class\n",
    "        - each unit gives the probabability of one class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example: Yahoo News Ranked Multilabel Learning dataset (http://research.yahoo.com)\n",
    "  - A subset is selected\n",
    "  - 4 classes, 6426 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'columbia laboratories stockholders approve sale of progesterone assets to watson pharmaceuticals yahoo finance what are streaming quotes market name u s id us market tz edt tzoffset open close flags streamer server http streamerapi finance yahoo com arrowaschangesign false throttleinterval skip navigation home investing market overview market stats stocks mutual funds etfs bonds options industries currencies education news opinion markets investing ideas expert advice special editions company finances providers videonew personal finance banking budgeting career work college education family home insurance loans real estate retirement taxes lifelong investing my portfolios sign in to access my portfolios free trial of real time quotes tech ticker search for share prices search for share prices finance searchthu aug edt u s markets close in mins columbia laboratories stockholders approve sale of progesterone assets to watson pharmaceuticals companies columbia laboratories inc related s cbrx k o j press release source columbia laboratories inc on thursday july am edt livingston n j business wire columbia laboratories inc nasdaq cbrx news announced that in a special meeting of stockholders held this morning stockholders approved the sale of substantially all of columbia s progesterone related assets to watson pharmaceuticals inc nyse wpi news the watson transaction stockholders also voted to increase the number of authorized shares of columbia s common stock par value per share from to the charter amendment we are pleased that our stockholders have approved the sale of our progesterone assets to watson and the share increase necessary to complete that transaction said frank c condella jr columbia s president and chief executive officer we look forward to closing this transaction in the next few days and using some of the initial proceeds to immediately retire our debt this is the first step in columbia s transformation to a focused development company debt free with a clearer path to profitability as previously announced on march columbia entered into a definitive agreement to sell substantially all of its progesterone related assets including its preterm birth patent applications and million shares of newly issued common stock to watson columbia will receive million upfront from watson and watson will forgive all principal and accrued interest on the million subordinated term loan dated june columbia will also receive royalties of to percent of annual net sales of certain progesterone products and is eligible for an additional amount of up to million based on success milestones in the potential preterm birth indication watson will fund the development of a second generation vaginal progesterone product as part of a comprehensive life cycle management strategy columbia will retain certain assets and rights to its progesterone business including all rights necessary to perform its obligations under its agreement with merck serono s a merck serono holds marketing rights to and makes payments to columbia related to crinone progesterone gel sales in all countries outside the united states the watson transaction is expected to close within three business days about crinone prochievecrinone progesterone gel is currently used for progesterone supplementation or replacement as part of an assisted reproductive technology art treatment for infertile women with a progesterone deficiency patient preference for crinone has been demonstrated in five clinical trials this product is also available under the trade name prochieve for more information please visit www crinoneusa com the most common side effects of crinone prochieve include breast enlargement constipation somnolence nausea headache and perineal pain crinone prochieve is contraindicated in patients with active thrombophlebitis or thromboembolic disorders or a history of hormone associated thrombophlebitis or thromboembolic disorders missed abortion undiagnosed vaginal bleeding liver dysfunction or disease and known or suspected malignancy of the breast or genital organs about columbia laboratoriescolumbia laboratories inc is a specialty pharmaceutical company focused on developing products for the women s healthcare and endocrinology markets that use its novel bioadhesive drug delivery technology columbia is conducting a randomized double blind placebo controlled phase iii clinical program called the pregnant prochieve extending gestation a new therapy study to evaluate the safety and efficacy of prochieve progesterone gel to reduce the risk of preterm birth in women with a cervical length between and centimeters as measured by transvaginal ultrasound at mid pregnancy the primary endpoint of this study is a reduction in the incidence of preterm birth at less than or equal to weeks gestation vs placebo the company expects study results around the end of the year columbia s press releases and other company information are available at columbia s website at www columbialabs com and its investor relations website at www cbrxir com safe harbor statement under the private securities litigation reform act of this communication contains forward looking statements which statements are indicated by the words may will plans believes expects anticipates potential and similar expressions such forward looking statements involve known and unknown risks uncertainties and other factors that may cause actual results to differ materially from those projected in the forward looking statements readers are cautioned not to place undue reliance on these forward looking statements which speak only as of the date on which they are made factors that might cause future results to differ include but are not limited to the following the successful marketing of crinone and striant in the united states the successful marketing of crinone by merck serono outside the united states the timely and successful completion of the ongoing phase iii pregnant prochieve extending gestation a new therapy study of prochieve to reduce the risk of preterm birth in women with a short cervix at mid pregnancy successful development of a next generation vaginal progesterone product success in obtaining acceptance and approval of new products and new indications for current products by the united states food and drug administration and international regulatory agencies the impact of competitive products and pricing our ability to obtain financing in order to fund our operations and repay our debt as it becomes due the timely and successful negotiation of partnerships or other transactions the strength of the united states dollar relative to international currencies particularly the euro competitive economic and regulatory factors in the pharmaceutical and healthcare industry general economic conditions and other risks and uncertainties that may be detailed from time to time in columbia s reports filed with the sec completion of the sale of the assets under the purchase and collaboration agreement with watson pharmaceuticals inc and the other transactions disclosed in the company s press release dated march and in the definitive proxy statement filed with the sec on june are subject to various conditions to closing and there can be no assurance those conditions will be satisfied or that such sale or other transactions will be completed on the terms described in the purchase and collaboration agreement with watson pharmaceuticals inc or other agreements related thereto or at all all forward looking statements contained herein are neither promises nor guarantees columbia does not undertake any responsibility to revise or update any forward looking statements contained herein crinone prochieve and striant are registered trademarks of columbia laboratories inc contact columbia laboratories inc lawrence a gyenes vice president chief financial officer treasurerorthe trout group llcseth lewis president related headlines columbia laboratories inc financials edgar online financials columbia laboratories inc files sec form k regulation fd disclosure financial statements and exhibits edgar online columbia laboratories inc files sec form k entry into a material definitive agreement edgar online columbia laboratories repurchases million shares of common stock business wire columbia laboratories inc files sec form k results of operations and financial condition financial statements and edgar online related message boards columbia laboratories inc there are no comments yet post a comment sign in to post a comment or sign up for a free account sponsored links million stock market winner california option trader makes million in stock market in year www virtualinvestingclub com finance investing stock market trade stocks with optionsxpress free tools to help with your trade optionsxpress com gold stocks for free gold report the easiest ways to make gains www taipanpublishinggroup com stock market trades start stock trading today search around you now aroundme com top stories wall street extends losses after cisco earnings jobless data ap gm posts big profit ceo says he s done his job ap bp to pay record fine for texas refinery problems reuters mortgage rates hit low of pct ap tech ticker recent posts deflation s coming says gary shilling and it s going to clobber the stock market henry blodget obama falls short gary shilling sees years of low growth rising unemployment aaron task michael pento favors depression vs more stimulus dan greenhaus can t believe it peter gorenstein view more subscribe to topics top stories add alert cbrx headlines add alert see all rss links copyright business wire all rights reserved all the news releases provided by business wire are copyrighted any forms of copying other than an individual user s personal reference without express written permission is prohibited further distribution of these materials by posting archiving in a public web site or database or redistribution in a computer network is strictly forbidden yahoo finance banking budgeting calculators currency etfs experts investing insurance market stats message boards mobile personal finance what s new also on yahoo autos finance flickr games groups health hot jobs mail maps movies music my yahoo news shopping sports travel tv video all y services things to do read our blog finance on your phone check home values find a new car search jobs across the web yahoo finance worldwide argentina australia brazil canada china chinese france french canada germany hong kong india italy japan korea mexico new zealand singapore spain spanish taiwan uk ireland usa quotes delayed except where indicated otherwise delay times are mins for nasdaq nyse and amex see also delay times for other exchanges quotes and other information supplied by independent providers identified on the yahoo finance partner page quotes are updated automatically but will be turned off after minutes of inactivity quotes are delayed at least minutes all information provided as is for informational purposes only not intended for trading purposes or advice neither yahoo nor any of independent providers is liable for any informational errors incompleteness or delays or for any actions taken in reliance on information contained herein by accessing the yahoo site you agree not to redistribute the information found therein fundamental company data provided by capital iq historical chart data and daily updates provided by commodity systems inc csi international historical chart data daily updates fundanalyst estimates data provided by thomson financial network all data povided by thomson financial network is based solely upon research information provided by third party analysts yahoo has not reviewed and in no way endorses the validity of such data yahoo and thomsonfn shall not be liable for any actions taken in reliance thereon\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['money', 'investment-&-company-information', 'investment']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 5.7.1: Load and process the data\n",
    "\n",
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load the data\n",
    "data=json.load(open(\"ydata.json\",'rb'))\n",
    "#data=json.load(open(\"ydata.json\",'r'))\n",
    "\n",
    "\n",
    "# shuffle the data\n",
    "shuffle(data)\n",
    "\n",
    "\n",
    "# split into text and label\n",
    "text,labels=zip(*data)\n",
    "text=list(text)\n",
    "labels=list(labels)\n",
    "text[1]\n",
    "labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6426, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(['crime-&-justice', 'investment',\n",
       "       'investment-&-company-information', 'money'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([2515, 3607, 3468, 3202])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 5.7.2: create indicator matrix for labels\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y=mlb.fit_transform(labels)\n",
    "# check size of indicator matrix\n",
    "Y.shape\n",
    "# check classes\n",
    "mlb.classes_\n",
    "# check # of samples in each class\n",
    "np.sum(Y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.7.3: Load and process the data\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "# get a Keras tokenizer\n",
    "\n",
    "MAX_NB_WORDS=8000\n",
    "# documents are quite long in the dataset\n",
    "MAX_DOC_LEN=1000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text)\n",
    "voc=tokenizer.word_index\n",
    "# convert each document to a list of word index as a sequence\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "# get the mapping between words to word index\n",
    "\n",
    "# pad all sequences into the same length (the longest)\n",
    "padded_sequences = pad_sequences(sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', truncating='post')\n",
    "\n",
    "#print(padded_sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5140 samples, validate on 1286 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.22730, saving model to best_model\n",
      "129s - loss: 0.4585 - acc: 0.7852 - val_loss: 0.2273 - val_acc: 0.9102\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.22730 to 0.18670, saving model to best_model\n",
      "128s - loss: 0.2274 - acc: 0.9190 - val_loss: 0.1867 - val_acc: 0.9358\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18670 to 0.17672, saving model to best_model\n",
      "323s - loss: 0.1724 - acc: 0.9391 - val_loss: 0.1767 - val_acc: 0.9393\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17672 to 0.17384, saving model to best_model\n",
      "137s - loss: 0.1381 - acc: 0.9524 - val_loss: 0.1738 - val_acc: 0.9425\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "139s - loss: 0.1189 - acc: 0.9578 - val_loss: 0.1856 - val_acc: 0.9388\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.7.4: Fit the model using the function\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "EMBEDDING_DIM=100\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "# set the number of output units\n",
    "# as the number of classes\n",
    "output_units_num=len(mlb.classes_)\n",
    "num_filters=64\n",
    "\n",
    "# set the dense units\n",
    "dense_units_num= num_filters*len(FILTER_SIZES)\n",
    "\n",
    "\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 20\n",
    "\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                padded_sequences, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, MAX_DOC_LEN, NUM_FILTERS=num_filters,\\\n",
    "                NUM_OUTPUT_UNITS=output_units_num, \\\n",
    "                NUM_DENSE_UNITS=dense_units_num)\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_loss', \\\n",
    "                             verbose=2, save_best_only=True, mode='min')\n",
    "    \n",
    "training=model.fit(X_train, Y_train, \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\\\n",
    "          validation_data=[X_test, Y_test], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9998558e-01, 3.8914490e-04, 9.1924172e-05, 2.1627230e-04],\n",
       "       [7.6253116e-03, 8.7126344e-01, 9.4516116e-01, 8.9557737e-01],\n",
       "       [7.8014191e-04, 9.3820077e-01, 9.7588003e-01, 9.2896700e-01],\n",
       "       [1.3660673e-04, 9.6760023e-01, 9.9078321e-01, 9.6447533e-01],\n",
       "       [9.9924588e-01, 6.9322013e-03, 2.5319203e-03, 4.4500292e-03]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 5.7.5: predicate using the best model\n",
    "# calculate performance\n",
    "\n",
    "# load the best model\n",
    "model.load_weights(\"best_model\")\n",
    "\n",
    "pred=model.predict(X_test)\n",
    "pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                 crime-&-justice       0.99      0.97      0.98       487\n",
      "                      investment       0.91      0.99      0.95       737\n",
      "investment-&-company-information       0.94      0.97      0.95       712\n",
      "                           money       0.85      0.96      0.90       651\n",
      "\n",
      "                     avg / total       0.92      0.97      0.95      2587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.7.6: Generate performance report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = np.where(pred>0.5,1,0)\n",
    "\n",
    "print(classification_report(Y_test,pred,target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.  Use Pretrained Word Vectors\n",
    "- If **the size of labeled samples is small, it's better use pretrained word vectors** \n",
    "    - e.g. google or facebook pretrained word vectors\n",
    "    - or you can train word vectors using relevant context data using gensim\n",
    "- Procedure:\n",
    "    1. Obtain/train pretrained word vectors (see Section 4.1 and Exercise 4.1.1)\n",
    "    2. Look for the word vector for each word in the vocabulary and create **embedding matrix** where each row represents one word vector\n",
    "    3. Set embedding layer with the embedding matrix and set it not trainable.\n",
    "- With well-trained word vectors, often a small sample set can also achieve good performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.8.1: Load full yahoo news dataset\n",
    "# to train the word vector\n",
    "# note this data can be unlabeled. only text is used\n",
    "import json\n",
    "\n",
    "data=json.load(open(\"ydata.json\",'r'))\n",
    "text,labels=zip(*data)\n",
    "text=list(text)\n",
    "\n",
    "sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "             for token in nltk.word_tokenize(doc) \\\n",
    "                 if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2]\\\n",
    "             for doc in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.8.2: Train word vector using \n",
    "# the large data set\n",
    "\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# print out tracking information\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "EMBEDDING_DIM=200\n",
    "# min_count: words with total frequency lower than this are ignored\n",
    "# size: the dimension of word vector\n",
    "# window: is the maximum distance \n",
    "#         between the current and predicted word \n",
    "#         within a sentence (i.e. the length of ngrams)\n",
    "# workers: # of parallel threads in training\n",
    "# for other parameters, check https://radimrehurek.com/gensim/models/word2vec.html\n",
    "wv_model = word2vec.Word2Vec(sentences, min_count=5, \\\n",
    "                             size=EMBEDDING_DIM, window=5, workers=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word vector for all words in the vocabulary\n",
    "# see reference at https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "\n",
    "EMBEDDING_DIM=200\n",
    "MAX_NB_WORDS=8000\n",
    "\n",
    "# tokenizer.word_index provides the mapping \n",
    "# between a word and word index for all words\n",
    "NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "\n",
    "# \"+1\" is for padding symbol\n",
    "embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    # if word_index is above the max number of words, ignore it\n",
    "    if i >= NUM_WORDS:\n",
    "        continue\n",
    "    if word in wv_model.wv:\n",
    "        embedding_matrix[i]=wv_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_loss improved from inf to 0.47023, saving model to best_model\n",
      "8s - loss: 1.1196 - acc: 0.5731 - val_loss: 0.4702 - val_acc: 0.7900\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_loss improved from 0.47023 to 0.35773, saving model to best_model\n",
      "7s - loss: 0.6822 - acc: 0.7112 - val_loss: 0.3577 - val_acc: 0.8450\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_loss did not improve\n",
      "10s - loss: 0.5356 - acc: 0.7994 - val_loss: 0.3872 - val_acc: 0.8600\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_loss improved from 0.35773 to 0.33921, saving model to best_model\n",
      "9s - loss: 0.4025 - acc: 0.8425 - val_loss: 0.3392 - val_acc: 0.8775\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_loss did not improve\n",
      "11s - loss: 0.3584 - acc: 0.8675 - val_loss: 0.3766 - val_acc: 0.8775\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_loss improved from 0.33921 to 0.33093, saving model to best_model\n",
      "8s - loss: 0.3095 - acc: 0.8856 - val_loss: 0.3309 - val_acc: 0.8825\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_loss improved from 0.33093 to 0.32157, saving model to best_model\n",
      "8s - loss: 0.3052 - acc: 0.8837 - val_loss: 0.3216 - val_acc: 0.8900\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_loss improved from 0.32157 to 0.28987, saving model to best_model\n",
      "8s - loss: 0.2480 - acc: 0.9050 - val_loss: 0.2899 - val_acc: 0.8950\n",
      "Epoch 9/100\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.2485 - acc: 0.9094 - val_loss: 0.2958 - val_acc: 0.8925\n",
      "Epoch 10/100\n",
      "Epoch 00009: val_loss improved from 0.28987 to 0.28635, saving model to best_model\n",
      "8s - loss: 0.2397 - acc: 0.9025 - val_loss: 0.2864 - val_acc: 0.9000\n",
      "Epoch 11/100\n",
      "Epoch 00010: val_loss did not improve\n",
      "8s - loss: 0.2583 - acc: 0.9019 - val_loss: 0.3007 - val_acc: 0.8900\n",
      "Epoch 12/100\n",
      "Epoch 00011: val_loss improved from 0.28635 to 0.28559, saving model to best_model\n",
      "9s - loss: 0.2130 - acc: 0.9119 - val_loss: 0.2856 - val_acc: 0.8950\n",
      "Epoch 13/100\n",
      "Epoch 00012: val_loss did not improve\n",
      "9s - loss: 0.2485 - acc: 0.8981 - val_loss: 0.3299 - val_acc: 0.8900\n",
      "Epoch 14/100\n",
      "Epoch 00013: val_loss did not improve\n",
      "8s - loss: 0.2060 - acc: 0.9200 - val_loss: 0.3292 - val_acc: 0.8875\n",
      "Epoch 00013: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.8.3: Fit model using pretrained word vectors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "EMBEDDING_DIM=200\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "# set the number of output units\n",
    "# as the number of classes\n",
    "output_units_num=len(mlb.classes_)\n",
    "\n",
    "#Number of filters for each size\n",
    "num_filters=64\n",
    "\n",
    "# set the dense units\n",
    "dense_units_num= num_filters*len(FILTER_SIZES)\n",
    "\n",
    "BTACH_SIZE = 32\n",
    "NUM_EPOCHES = 100\n",
    "\n",
    "# With well trained word vectors, sample size can be reduced\n",
    "# Assume we only have 500 labeled data\n",
    "# split dataset into train (70%) and test sets (20%)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                padded_sequences[0:500], Y[0:500], \\\n",
    "                test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "# create the model with embedding matrix\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, \\\n",
    "                NUM_FILTERS=num_filters,\\\n",
    "                NUM_OUTPUT_UNITS=output_units_num, \\\n",
    "                NUM_DENSE_UNITS=dense_units_num,\\\n",
    "                PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=1, verbose=2, mode='min')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_loss', \\\n",
    "                             verbose=2, save_best_only=True, mode='min')\n",
    "    \n",
    "training=model.fit(X_train, Y_train, \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\\\n",
    "          validation_data=[X_test, Y_test], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding (Embedding)            (None, 1000, 200)     1600200     main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv_2 (Conv1D)                  (None, 999, 64)       25664       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_3 (Conv1D)                  (None, 998, 64)       38464       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_4 (Conv1D)                  (None, 997, 64)       51264       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_2 (MaxPooling1D)             (None, 1, 64)         0           conv_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_3 (MaxPooling1D)             (None, 1, 64)         0           conv_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_4 (MaxPooling1D)             (None, 1, 64)         0           conv_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "flat_2 (Flatten)                 (None, 64)            0           max_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_3 (Flatten)                 (None, 64)            0           max_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_4 (Flatten)                 (None, 64)            0           max_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "concate (Concatenate)            (None, 192)           0           flat_2[0][0]                     \n",
      "                                                                   flat_3[0][0]                     \n",
      "                                                                   flat_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 192)           0           concate[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense (Dense)                    (None, 192)           37056       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 4)             772         dense[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,753,420\n",
      "Trainable params: 153,220\n",
      "Non-trainable params: 1,600,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.8.4: check model configuration\n",
    "# Note that parameters from embedding layer\n",
    "# is not trainable\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 1, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 1, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                 crime-&-justice       0.99      0.91      0.95       210\n",
      "                      investment       0.88      0.96      0.91       267\n",
      "investment-&-company-information       0.86      0.97      0.91       262\n",
      "                           money       0.79      0.95      0.86       241\n",
      "\n",
      "                     avg / total       0.88      0.95      0.91       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.8.5: Performance evaluation\n",
    "# Let's use samples[500:1000]\n",
    "# as an evaluation set\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "pred=model.predict(padded_sequences[500:1000])\n",
    "\n",
    "Y_pred=np.copy(pred)\n",
    "Y_pred=np.where(Y_pred>0.5,1,0)\n",
    "\n",
    "Y_pred[0:10]\n",
    "Y[500:510]\n",
    "\n",
    "print(classification_report(Y[500:1000], Y_pred, target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Note that we only trained the model with **500 samples**\n",
    "- The performance is only slightly lower, compared with the one trained with 6000 samples\n",
    "- This shows that pre-trained word vectors can effectively improve the classification performance in the case of small labeled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9. How to select hyperparameters?\n",
    "- Fitting a neural network is a very empirical process\n",
    "- See Section 3 of \"Practical Recommendations for Gradient-Based Training of Deep Architectures\" (https://arxiv.org/abs/1206.5533) for detailed discussion\n",
    "- The following is some useful techniques to set \n",
    "  - MAX_NB_WORDS: max number words to be included in word embedding\n",
    "    - Based on word frequency histogram to include words that appear at least $n$ times\n",
    "  - MAX_DOC_LEN: max length of documents\n",
    "    - Based on document length frequency histogram to include complete sentences as many as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              freq\n",
      "jetblue        179\n",
      "expands         93\n",
      "service       2404\n",
      "to           83098\n",
      "connecticut    105\n",
      "   word_freq  count   percent    cumsum\n",
      "0          1  25112  0.378238  0.378238\n",
      "1          2   9216  0.138812  0.517050\n",
      "2          3   4842  0.072930  0.589981\n",
      "3          4   3254  0.049012  0.638993\n",
      "4          5   2367  0.035652  0.674645\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAELCAYAAAAiIMZEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c4b82c668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exercise 5.9.1 Set MAX_NB_WORDS to \n",
    "# include words that appear at least K times\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# get count of each word\n",
    "df=pd.DataFrame.from_dict(tokenizer.word_counts, orient=\"index\")\n",
    "df.columns=['freq']\n",
    "print(df.head())\n",
    "\n",
    "# get histogram of word count\n",
    "df=df['freq'].value_counts().reset_index()\n",
    "df.columns=['word_freq','count']\n",
    "\n",
    "# sort by word_freq\n",
    "df=df.sort_values(by='word_freq')\n",
    "\n",
    "# convert absolute counts to precentage\n",
    "df['percent']=df['count']/len(tokenizer.word_counts)\n",
    "# get cumulative percentage\n",
    "df['cumsum']=df['percent'].cumsum()\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df.iloc[0:50].plot(x='word_freq', y='cumsum');\n",
    "\n",
    "plt.show();\n",
    "\n",
    "# if set min count for word to 10, \n",
    "# what % of words can be included?\n",
    "# how many words will be included?\n",
    "# This is the parameter MAX_NB_WORDS\n",
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.9.2 Set MAX_DOC_LEN to \n",
    "# include complete sentences as many as possible\n",
    "\n",
    "# create a series based on the length of all sentences\n",
    "sen_len=pd.Series([len(item) for item in sequences])\n",
    "\n",
    "# create histogram of sentence length\n",
    "# the \"index\" is the sentence length\n",
    "# \"counts\" is the count of sentences at a length\n",
    "df=sen_len.value_counts().reset_index().sort_values(by='index')\n",
    "df.columns=['sent_length','counts']\n",
    "\n",
    "# sort by sentence length\n",
    "# get percentage and cumulative percentage\n",
    "\n",
    "df['percent']=df['counts']/len(sen_len)\n",
    "df['cumsum']=df['percent'].cumsum()\n",
    "print(df.head(3))\n",
    "\n",
    "# From the plot, 90% sentences have length<500\n",
    "# so it makes sense to set MAX_DOC_LEN=4~500 \n",
    "df.plot(x=\"sent_length\", y='cumsum');\n",
    "plt.show();\n",
    "\n",
    "# what will be the minimum sentence length\n",
    "# such that 99% of sentences will not be truncated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
